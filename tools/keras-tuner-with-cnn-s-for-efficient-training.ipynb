{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Updating pip and installing keras-tuner package\n!/opt/conda/bin/python3.7 -m pip install --upgrade pip\n!pip install -U keras-tuner","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pip\n  Downloading pip-20.1.1-py2.py3-none-any.whl (1.5 MB)\n\u001b[K     |████████████████████████████████| 1.5 MB 2.9 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 20.1\n    Uninstalling pip-20.1:\n      Successfully uninstalled pip-20.1\nSuccessfully installed pip-20.1.1\nCollecting keras-tuner\n  Downloading keras-tuner-1.0.1.tar.gz (54 kB)\n\u001b[K     |████████████████████████████████| 54 kB 1.0 MB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied, skipping upgrade: future in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.18.2)\nRequirement already satisfied, skipping upgrade: numpy in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (1.18.1)\nRequirement already satisfied, skipping upgrade: tabulate in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.8.7)\nCollecting terminaltables\n  Downloading terminaltables-3.1.0.tar.gz (12 kB)\nRequirement already satisfied, skipping upgrade: colorama in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.4.3)\nRequirement already satisfied, skipping upgrade: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (4.45.0)\nRequirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (2.23.0)\nRequirement already satisfied, skipping upgrade: scipy in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (1.4.1)\nRequirement already satisfied, skipping upgrade: scikit-learn in /opt/conda/lib/python3.7/site-packages (from keras-tuner) (0.22.2.post1)\nRequirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2.9)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (1.24.3)\nRequirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (3.0.4)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->keras-tuner) (2020.4.5.1)\nRequirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->keras-tuner) (0.14.1)\nBuilding wheels for collected packages: keras-tuner, terminaltables\n  Building wheel for keras-tuner (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-py3-none-any.whl size=73198 sha256=7962a4980603548ed9c6715b5d1800e21ea323a0b20444950561e7ab22d3db44\n  Stored in directory: /root/.cache/pip/wheels/0b/cf/2f/1a1749d3a3650fac3305a8d7f9237b6de7c41068e2f8520ca2\n  Building wheel for terminaltables (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15354 sha256=86cb18b0e31f677ca2dd2d0a792b837c0440a8af36333ccd3f9de6c0da802e69\n  Stored in directory: /root/.cache/pip/wheels/ba/ad/c8/2d98360791161cd3db6daf6b5e730f34021fc9367d5879f497\nSuccessfully built keras-tuner terminaltables\nInstalling collected packages: terminaltables, keras-tuner\nSuccessfully installed keras-tuner-1.0.1 terminaltables-3.1.0\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom kerastuner import HyperModel, Objective\nimport tensorflow as tf\nfrom kerastuner.tuners import RandomSearch\nimport keras.backend as K\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":2,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Initializing the embedding dimention\nbatch_size = 64\nembedding_dim = 512\n\n# Read the input data.\ntrain_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\n# Fill all the na data with empty character.\ntrain_df = train_df.fillna('empty')\ntest_df = test_df.fillna('empty')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Join with the text, keyword as well as the location\ntrain_df['text'] = train_df['text'] + ' ' + train_df['keyword'].astype(str) + ' ' + train_df['location'].astype(str)\ntest_df['text'] = test_df['text'] + ' ' + test_df['keyword'].astype(str) + ' ' + test_df['location'].astype(str)\n\n# Strip off the whitespace from the front and back of the sentence\ntrain_df['text'] = train_df['text'].str.strip()\ntest_df['text'] = test_df['text'].str.strip()\n\n# Replace all the links, with just link as word.\n# It matters more to know if there is a link or not in place of which link it is actually.\n# But this remains to be seen later.\ntrain_df['text'] = train_df['text'].str.replace(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'link')\ntest_df['text'] = test_df['text'].str.replace(r'(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?', 'link')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# To see the output of the pre-processing steps.\ntrain_df.to_csv('train.csv', index=False)\n\n# Drop the kexword and location column as it is not with the text column\ntrain_df.drop(columns=['keyword', 'location'])\n\n# Use the tokenizer and remove all the special characters. Add oov_character as irrelevant\ntokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=\"'<irrelevant>'\",\n                                                  filters='!\"$%&()*+.,-/:;=?@[\\]^_`{|}~# ')\n\n# Fit the tokenizer on the trainig data.\ntokenizer.fit_on_texts(train_df.text)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert the texts to tokens\ntrain_df['text_tokenized'] = tokenizer.texts_to_sequences(train_df.text)\ntest_df['text_tokenized'] = tokenizer.texts_to_sequences(test_df.text)\n\n# Pad the sequence as we will like to have sentences of equal length.\n# We can also use bucket with sequence length.\nnp_matrix_train = tf.keras.preprocessing.sequence.pad_sequences(train_df['text_tokenized'])\nnp_matrix_train = np.append(np_matrix_train, np.expand_dims(train_df['target'], axis=-1), axis=1)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the data to x and y, Later batch the dataset.\ntrain_dataset = tf.data.Dataset.from_tensor_slices(np_matrix_train)\ntrain_dataset_all = train_dataset.map(lambda x: (x[:-1], x[-1])).batch(batch_size)\n\n\n\n# Do the similar to the test dataset.\nnp_matrix_test = tf.keras.preprocessing.sequence.pad_sequences(test_df['text_tokenized'])\ntest_dataset = tf.data.Dataset.from_tensor_slices(np_matrix_test).batch(1)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def is_test(x, y):\n    return x % 5 == 0\n\ndef is_train(x, y):\n    return not is_test(x, y)\n\nrecover = lambda x,y: y\n\nvalidation_dataset = train_dataset_all.enumerate() \\\n                    .filter(is_test) \\\n                    .map(recover)\n\ntrain_dataset = train_dataset_all.enumerate() \\\n                    .filter(is_train) \\\n                    .map(recover)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!rm -rf  ./real_or_not*","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp / (tp + fp + K.epsilon())\n    r = tp / (tp + fn + K.epsilon())\n\n    f1 = 2*p*r / (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the keras tuner model.\nclass MyHyperModel(HyperModel):\n    \n    def build(self, hp):\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Embedding(len(tokenizer.word_index) + 1, embedding_dim))\n        for i in range(hp.Int('num_layers', 1, 3)):\n            model.add(tf.keras.layers.Conv1D(filters=hp.Choice('num_filters', values=[32, 64], default=64),activation='relu',\n                                             kernel_size=3,\n                                             bias_initializer='glorot_uniform'))\n            model.add(tf.keras.layers.MaxPool1D())\n        \n        model.add(tf.keras.layers.GlobalMaxPool1D())\n        \n        for i in range(hp.Int('num_layers_rnn', 1, 3)):\n            model.add(tf.keras.layers.Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu'))\n            model.add(tf.keras.layers.Dropout(0.2))\n        \n        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n        \n        model.compile(\n            optimizer=hp.Choice('optimizer', values= ['Adam', 'Adadelta', 'Adamax']),\n            loss='binary_crossentropy',\n            metrics=[f1])\n        return model\n\n\nhypermodel = MyHyperModel()\n\ntuner = RandomSearch(\n    hypermodel,\n    objective=Objective('val_f1', direction=\"max\"),\n    max_trials=15,\n    directory='./',\n    project_name='real_or_not')\n\ntuner.search(train_dataset,\n             epochs=10, validation_data=validation_dataset)\n","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/10\n95/95 [==============================].6941 - f1: 0.56 - 1s 616ms/step - loss: 0.6922 - f1: 0.644 - 1s 429ms/step - loss: 0.6952 - f1: 0.592 - 1s 336ms/step - loss: 0.6968 - f1: 0.545 - 1s 278ms/step - loss: 0.6967 - f1: 0.556 - 1s 239ms/step - loss: 0.6954 - f1: 0.585 - 1s 212ms/step - loss: 0.6970 - f1: 0.549 - 2s 192ms/step - loss: 0.6961 - f1: 0.577 - 2s 177ms/step - loss: 0.6981 - f1: 0.527 - 2s 165ms/step - loss: 0.6997 - f1: 0.487 - 2s 155ms/step - loss: 0.7006 - f1: 0.461 - 2s 147ms/step - loss: 0.7014 - f1: 0.438 - 2s 139ms/step - loss: 0.7017 - f1: 0.429 - 2s 132ms/step - loss: 0.7005 - f1: 0.451 - 2s 126ms/step - loss: 0.6995 - f1: 0.475 - 2s 121ms/step - loss: 0.6987 - f1: 0.490 - 2s 116ms/step - loss: 0.6980 - f1: 0.504 - 2s 112ms/step - loss: 0.6978 - f1: 0.510 - 2s 108ms/step - loss: 0.6978 - f1: 0.512 - 2s 104ms/step - loss: 0.6981 - f1: 0.508 - 2s 101ms/step - loss: 0.6982 - f1: 0.504 - 2s 99ms/step - loss: 0.6976 - f1: 0.515 - 2s 96ms/step - loss: 0.6975 - f1: 0.51 - 2s 94ms/step - loss: 0.6981 - f1: 0.50 - 2s 92ms/step - loss: 0.6983 - f1: 0.50 - 2s 90ms/step - loss: 0.6984 - f1: 0.50 - 2s 88ms/step - loss: 0.6980 - f1: 0.51 - 2s 86ms/step - loss: 0.6979 - f1: 0.51 - 2s 84ms/step - loss: 0.6980 - f1: 0.51 - 2s 83ms/step - loss: 0.6979 - f1: 0.51 - 3s 82ms/step - loss: 0.6974 - f1: 0.52 - 3s 80ms/step - loss: 0.6975 - f1: 0.52 - 3s 79ms/step - loss: 0.6977 - f1: 0.51 - 3s 78ms/step - loss: 0.6977 - f1: 0.51 - 3s 77ms/step - loss: 0.6976 - f1: 0.52 - 3s 76ms/step - loss: 0.6971 - f1: 0.53 - 3s 75ms/step - loss: 0.6970 - f1: 0.53 - 3s 74ms/step - loss: 0.6967 - f1: 0.53 - 3s 73ms/step - loss: 0.6970 - f1: 0.53 - 3s 72ms/step - loss: 0.6969 - f1: 0.53 - 3s 71ms/step - loss: 0.6967 - f1: 0.53 - 3s 70ms/step - loss: 0.6962 - f1: 0.54 - 3s 70ms/step - loss: 0.6964 - f1: 0.54 - 3s 69ms/step - loss: 0.6965 - f1: 0.54 - 3s 68ms/step - loss: 0.6964 - f1: 0.54 - 3s 68ms/step - loss: 0.6965 - f1: 0.54 - 3s 67ms/step - loss: 0.6967 - f1: 0.53 - 3s 66ms/step - loss: 0.6966 - f1: 0.54 - 3s 66ms/step - loss: 0.6965 - f1: 0.54 - 3s 65ms/step - loss: 0.6962 - f1: 0.54 - 3s 65ms/step - loss: 0.6961 - f1: 0.55 - 3s 64ms/step - loss: 0.6959 - f1: 0.55 - 3s 64ms/step - loss: 0.6958 - f1: 0.55 - 3s 63ms/step - loss: 0.6958 - f1: 0.55 - 3s 63ms/step - loss: 0.6958 - f1: 0.55 - 4s 63ms/step - loss: 0.6956 - f1: 0.56 - 4s 62ms/step - loss: 0.6957 - f1: 0.56 - 4s 62ms/step - loss: 0.6958 - f1: 0.55 - 4s 61ms/step - loss: 0.6959 - f1: 0.55 - 4s 61ms/step - loss: 0.6960 - f1: 0.55 - 4s 61ms/step - loss: 0.6959 - f1: 0.55 - 4s 60ms/step - loss: 0.6961 - f1: 0.55 - 4s 60ms/step - loss: 0.6962 - f1: 0.55 - 4s 60ms/step - loss: 0.6961 - f1: 0.55 - 4s 59ms/step - loss: 0.6961 - f1: 0.55 - 4s 59ms/step - loss: 0.6958 - f1: 0.55 - 4s 59ms/step - loss: 0.6959 - f1: 0.55 - 4s 58ms/step - loss: 0.6961 - f1: 0.55 - 4s 58ms/step - loss: 0.6961 - f1: 0.55 - 4s 58ms/step - loss: 0.6959 - f1: 0.55 - 4s 58ms/step - loss: 0.6959 - f1: 0.55 - 4s 58ms/step - loss: 0.6958 - f1: 0.55 - 4s 57ms/step - loss: 0.6959 - f1: 0.55 - 4s 57ms/step - loss: 0.6959 - f1: 0.55 - 4s 57ms/step - loss: 0.6961 - f1: 0.55 - 4s 57ms/step - loss: 0.6959 - f1: 0.55 - 4s 57ms/step - loss: 0.6960 - f1: 0.55 - 4s 57ms/step - loss: 0.6961 - f1: 0.54 - 4s 57ms/step - loss: 0.6961 - f1: 0.54 - 5s 57ms/step - loss: 0.6959 - f1: 0.54 - 5s 56ms/step - loss: 0.6960 - f1: 0.54 - 5s 56ms/step - loss: 0.6961 - f1: 0.54 - 5s 56ms/step - loss: 0.6959 - f1: 0.54 - 5s 56ms/step - loss: 0.6960 - f1: 0.54 - 5s 56ms/step - loss: 0.6959 - f1: 0.54 - 5s 55ms/step - loss: 0.6960 - f1: 0.54 - 5s 55ms/step - loss: 0.6961 - f1: 0.54 - 5s 55ms/step - loss: 0.6961 - f1: 0.54 - 5s 55ms/step - loss: 0.6960 - f1: 0.54 - 5s 55ms/step - loss: 0.6960 - f1: 0.54 - 5s 54ms/step - loss: 0.6961 - f1: 0.54 - 5s 54ms/step - loss: 0.6959 - f1: 0.55 - 5s 54ms/step - loss: 0.6959 - f1: 0.54 - 5s 54ms/step - loss: 0.6958 - f1: 0.55 - 5s 54ms/step - loss: 0.6958 - f1: 0.55 - 7s 70ms/step - loss: 0.6958 - f1: 0.5518 - val_loss: 0.6978 - val_f1: 0.5154\nEpoch 2/10\n95/95 [==============================] - ETA: 6s - loss: 0.6941 - f1: 0.51 - ETA: 4s - loss: 0.6933 - f1: 0.58 - ETA: 3s - loss: 0.6965 - f1: 0.54 - ETA: 3s - loss: 0.6957 - f1: 0.54 - ETA: 3s - loss: 0.6965 - f1: 0.51 - ETA: 3s - loss: 0.6988 - f1: 0.45 - ETA: 3s - loss: 0.7002 - f1: 0.42 - ETA: 3s - loss: 0.6981 - f1: 0.46 - ETA: 3s - loss: 0.6963 - f1: 0.50 - ETA: 2s - loss: 0.6961 - f1: 0.51 - ETA: 2s - loss: 0.6963 - f1: 0.51 - ETA: 2s - loss: 0.6959 - f1: 0.52 - ETA: 2s - loss: 0.6963 - f1: 0.51 - ETA: 2s - loss: 0.6963 - f1: 0.51 - ETA: 2s - loss: 0.6966 - f1: 0.51 - ETA: 2s - loss: 0.6961 - f1: 0.52 - ETA: 2s - loss: 0.6966 - f1: 0.51 - ETA: 2s - loss: 0.6963 - f1: 0.52 - ETA: 2s - loss: 0.6960 - f1: 0.52 - ETA: 2s - loss: 0.6962 - f1: 0.52 - ETA: 2s - loss: 0.6960 - f1: 0.53 - ETA: 2s - loss: 0.6957 - f1: 0.53 - ETA: 1s - loss: 0.6956 - f1: 0.53 - ETA: 1s - loss: 0.6958 - f1: 0.53 - ETA: 1s - loss: 0.6955 - f1: 0.53 - ETA: 1s - loss: 0.6952 - f1: 0.54 - ETA: 1s - loss: 0.6950 - f1: 0.55 - ETA: 1s - loss: 0.6950 - f1: 0.55 - ETA: 1s - loss: 0.6950 - f1: 0.55 - ETA: 1s - loss: 0.6953 - f1: 0.54 - ETA: 1s - loss: 0.6952 - f1: 0.54 - ETA: 1s - loss: 0.6954 - f1: 0.54 - ETA: 1s - loss: 0.6954 - f1: 0.54 - ETA: 1s - loss: 0.6951 - f1: 0.54 - ETA: 0s - loss: 0.6952 - f1: 0.54 - ETA: 0s - loss: 0.6950 - f1: 0.54 - ETA: 0s - loss: 0.6950 - f1: 0.54 - ETA: 0s - loss: 0.6951 - f1: 0.54 - ETA: 0s - loss: 0.6952 - f1: 0.54 - ETA: 0s - loss: 0.6953 - f1: 0.53 - ETA: 0s - loss: 0.6952 - f1: 0.53 - ETA: 0s - loss: 0.6952 - f1: 0.54 - ETA: 0s - loss: 0.6952 - f1: 0.54 - ETA: 0s - loss: 0.6954 - f1: 0.53 - ETA: 0s - loss: 0.6954 - f1: 0.54 - ETA: 0s - loss: 0.6954 - f1: 0.54 - ETA: 0s - loss: 0.6952 - f1: 0.54 - 4s 47ms/step - loss: 0.6952 - f1: 0.5459 - val_loss: 0.6967 - val_f1: 0.5154\nEpoch 3/10\n95/95 [==============================] - ETA: 6s - loss: 0.6936 - f1: 0.55 - ETA: 4s - loss: 0.6927 - f1: 0.60 - ETA: 3s - loss: 0.6953 - f1: 0.55 - ETA: 3s - loss: 0.6959 - f1: 0.54 - ETA: 3s - loss: 0.6961 - f1: 0.51 - ETA: 3s - loss: 0.6977 - f1: 0.46 - ETA: 3s - loss: 0.6987 - f1: 0.42 - ETA: 3s - loss: 0.6973 - f1: 0.46 - ETA: 3s - loss: 0.6962 - f1: 0.49 - ETA: 2s - loss: 0.6960 - f1: 0.50 - ETA: 2s - loss: 0.6964 - f1: 0.49 - ETA: 2s - loss: 0.6962 - f1: 0.50 - ETA: 2s - loss: 0.6966 - f1: 0.49 - ETA: 2s - loss: 0.6965 - f1: 0.49 - ETA: 2s - loss: 0.6965 - f1: 0.49 - ETA: 2s - loss: 0.6959 - f1: 0.51 - ETA: 2s - loss: 0.6963 - f1: 0.50 - ETA: 2s - loss: 0.6960 - f1: 0.50 - ETA: 2s - loss: 0.6956 - f1: 0.51 - ETA: 2s - loss: 0.6956 - f1: 0.51 - ETA: 2s - loss: 0.6956 - f1: 0.51 - ETA: 2s - loss: 0.6952 - f1: 0.52 - ETA: 1s - loss: 0.6955 - f1: 0.51 - ETA: 1s - loss: 0.6954 - f1: 0.52 - ETA: 1s - loss: 0.6955 - f1: 0.51 - ETA: 1s - loss: 0.6954 - f1: 0.52 - ETA: 1s - loss: 0.6953 - f1: 0.52 - ETA: 1s - loss: 0.6953 - f1: 0.53 - ETA: 1s - loss: 0.6952 - f1: 0.53 - ETA: 1s - loss: 0.6952 - f1: 0.52 - ETA: 1s - loss: 0.6954 - f1: 0.52 - ETA: 1s - loss: 0.6955 - f1: 0.52 - ETA: 1s - loss: 0.6955 - f1: 0.52 - ETA: 1s - loss: 0.6954 - f1: 0.52 - ETA: 1s - loss: 0.6956 - f1: 0.51 - ETA: 0s - loss: 0.6954 - f1: 0.52 - ETA: 0s - loss: 0.6953 - f1: 0.52 - ETA: 0s - loss: 0.6954 - f1: 0.52 - ETA: 0s - loss: 0.6954 - f1: 0.52 - ETA: 0s - loss: 0.6955 - f1: 0.51 - ETA: 0s - loss: 0.6955 - f1: 0.51 - ETA: 0s - loss: 0.6955 - f1: 0.51 - ETA: 0s - loss: 0.6953 - f1: 0.51 - ETA: 0s - loss: 0.6953 - f1: 0.52 - ETA: 0s - loss: 0.6953 - f1: 0.51 - ETA: 0s - loss: 0.6953 - f1: 0.51 - ETA: 0s - loss: 0.6953 - f1: 0.51 - ETA: 0s - loss: 0.6952 - f1: 0.52 - 5s 48ms/step - loss: 0.6951 - f1: 0.5236 - val_loss: 0.6957 - val_f1: 0.5154\nEpoch 4/10\n","name":"stdout"},{"output_type":"stream","text":"95/95 [==============================] - ETA: 7s - loss: 0.6995 - f1: 0.45 - ETA: 5s - loss: 0.6952 - f1: 0.51 - ETA: 4s - loss: 0.6949 - f1: 0.49 - ETA: 4s - loss: 0.6951 - f1: 0.49 - ETA: 4s - loss: 0.6954 - f1: 0.48 - ETA: 3s - loss: 0.6962 - f1: 0.43 - ETA: 3s - loss: 0.6966 - f1: 0.40 - ETA: 3s - loss: 0.6958 - f1: 0.43 - ETA: 3s - loss: 0.6954 - f1: 0.46 - ETA: 3s - loss: 0.6951 - f1: 0.47 - ETA: 3s - loss: 0.6953 - f1: 0.46 - ETA: 3s - loss: 0.6949 - f1: 0.47 - ETA: 2s - loss: 0.6953 - f1: 0.46 - ETA: 2s - loss: 0.6952 - f1: 0.46 - ETA: 2s - loss: 0.6952 - f1: 0.46 - ETA: 2s - loss: 0.6949 - f1: 0.47 - ETA: 2s - loss: 0.6949 - f1: 0.47 - ETA: 2s - loss: 0.6948 - f1: 0.48 - ETA: 2s - loss: 0.6946 - f1: 0.49 - ETA: 2s - loss: 0.6946 - f1: 0.49 - ETA: 2s - loss: 0.6946 - f1: 0.49 - ETA: 2s - loss: 0.6946 - f1: 0.49 - ETA: 2s - loss: 0.6946 - f1: 0.50 - ETA: 2s - loss: 0.6945 - f1: 0.50 - ETA: 2s - loss: 0.6945 - f1: 0.50 - ETA: 1s - loss: 0.6945 - f1: 0.50 - ETA: 1s - loss: 0.6945 - f1: 0.50 - ETA: 1s - loss: 0.6943 - f1: 0.51 - ETA: 1s - loss: 0.6943 - f1: 0.51 - ETA: 1s - loss: 0.6943 - f1: 0.51 - ETA: 1s - loss: 0.6943 - f1: 0.51 - ETA: 1s - loss: 0.6945 - f1: 0.51 - ETA: 1s - loss: 0.6945 - f1: 0.51 - ETA: 1s - loss: 0.6946 - f1: 0.51 - ETA: 1s - loss: 0.6946 - f1: 0.51 - ETA: 1s - loss: 0.6946 - f1: 0.51 - ETA: 1s - loss: 0.6947 - f1: 0.50 - ETA: 1s - loss: 0.6946 - f1: 0.51 - ETA: 0s - loss: 0.6946 - f1: 0.51 - ETA: 0s - loss: 0.6948 - f1: 0.51 - ETA: 0s - loss: 0.6948 - f1: 0.50 - ETA: 0s - loss: 0.6949 - f1: 0.50 - ETA: 0s - loss: 0.6949 - f1: 0.50 - ETA: 0s - loss: 0.6949 - f1: 0.49 - ETA: 0s - loss: 0.6949 - f1: 0.50 - ETA: 0s - loss: 0.6950 - f1: 0.49 - ETA: 0s - loss: 0.6950 - f1: 0.49 - ETA: 0s - loss: 0.6950 - f1: 0.49 - ETA: 0s - loss: 0.6949 - f1: 0.50 - ETA: 0s - loss: 0.6948 - f1: 0.50 - 5s 49ms/step - loss: 0.6948 - f1: 0.5043 - val_loss: 0.6948 - val_f1: 0.5152\nEpoch 5/10\n95/95 [==============================] - ETA: 5s - loss: 0.6969 - f1: 0.45 - ETA: 4s - loss: 0.6930 - f1: 0.55 - ETA: 3s - loss: 0.6931 - f1: 0.52 - ETA: 3s - loss: 0.6937 - f1: 0.51 - ETA: 3s - loss: 0.6941 - f1: 0.48 - ETA: 3s - loss: 0.6947 - f1: 0.43 - ETA: 3s - loss: 0.6952 - f1: 0.40 - ETA: 3s - loss: 0.6946 - f1: 0.44 - ETA: 3s - loss: 0.6942 - f1: 0.46 - ETA: 3s - loss: 0.6940 - f1: 0.47 - ETA: 2s - loss: 0.6941 - f1: 0.47 - ETA: 2s - loss: 0.6939 - f1: 0.49 - ETA: 2s - loss: 0.6943 - f1: 0.47 - ETA: 2s - loss: 0.6942 - f1: 0.48 - ETA: 2s - loss: 0.6944 - f1: 0.48 - ETA: 2s - loss: 0.6941 - f1: 0.49 - ETA: 2s - loss: 0.6942 - f1: 0.48 - ETA: 2s - loss: 0.6941 - f1: 0.49 - ETA: 2s - loss: 0.6940 - f1: 0.49 - ETA: 2s - loss: 0.6940 - f1: 0.49 - ETA: 2s - loss: 0.6938 - f1: 0.50 - ETA: 2s - loss: 0.6937 - f1: 0.50 - ETA: 1s - loss: 0.6936 - f1: 0.50 - ETA: 1s - loss: 0.6938 - f1: 0.50 - ETA: 1s - loss: 0.6939 - f1: 0.49 - ETA: 1s - loss: 0.6937 - f1: 0.50 - ETA: 1s - loss: 0.6937 - f1: 0.50 - ETA: 1s - loss: 0.6938 - f1: 0.50 - ETA: 1s - loss: 0.6938 - f1: 0.50 - ETA: 1s - loss: 0.6940 - f1: 0.49 - ETA: 1s - loss: 0.6941 - f1: 0.49 - ETA: 1s - loss: 0.6942 - f1: 0.49 - ETA: 1s - loss: 0.6941 - f1: 0.49 - ETA: 1s - loss: 0.6940 - f1: 0.49 - ETA: 1s - loss: 0.6941 - f1: 0.49 - ETA: 0s - loss: 0.6939 - f1: 0.49 - ETA: 0s - loss: 0.6939 - f1: 0.49 - ETA: 0s - loss: 0.6939 - f1: 0.49 - ETA: 0s - loss: 0.6940 - f1: 0.49 - ETA: 0s - loss: 0.6941 - f1: 0.48 - ETA: 0s - loss: 0.6940 - f1: 0.49 - ETA: 0s - loss: 0.6941 - f1: 0.48 - ETA: 0s - loss: 0.6940 - f1: 0.49 - ETA: 0s - loss: 0.6940 - f1: 0.49 - ETA: 0s - loss: 0.6941 - f1: 0.49 - ETA: 0s - loss: 0.6941 - f1: 0.48 - ETA: 0s - loss: 0.6940 - f1: 0.48 - ETA: 0s - loss: 0.6940 - f1: 0.49 - ETA: 0s - loss: 0.6940 - f1: 0.49 - 5s 47ms/step - loss: 0.6940 - f1: 0.4923 - val_loss: 0.6939 - val_f1: 0.4965\nEpoch 6/10\n95/95 [==============================] - ETA: 6s - loss: 0.6971 - f1: 0.38 - ETA: 4s - loss: 0.6955 - f1: 0.48 - ETA: 4s - loss: 0.6952 - f1: 0.47 - ETA: 3s - loss: 0.6942 - f1: 0.48 - ETA: 3s - loss: 0.6944 - f1: 0.47 - ETA: 3s - loss: 0.6952 - f1: 0.41 - ETA: 3s - loss: 0.6954 - f1: 0.39 - ETA: 3s - loss: 0.6952 - f1: 0.41 - ETA: 3s - loss: 0.6950 - f1: 0.43 - ETA: 2s - loss: 0.6945 - f1: 0.44 - ETA: 2s - loss: 0.6947 - f1: 0.43 - ETA: 2s - loss: 0.6949 - f1: 0.44 - ETA: 2s - loss: 0.6951 - f1: 0.43 - ETA: 2s - loss: 0.6948 - f1: 0.43 - ETA: 2s - loss: 0.6948 - f1: 0.43 - ETA: 2s - loss: 0.6946 - f1: 0.45 - ETA: 2s - loss: 0.6947 - f1: 0.43 - ETA: 2s - loss: 0.6946 - f1: 0.44 - ETA: 2s - loss: 0.6946 - f1: 0.45 - ETA: 2s - loss: 0.6945 - f1: 0.45 - ETA: 2s - loss: 0.6945 - f1: 0.45 - ETA: 1s - loss: 0.6946 - f1: 0.44 - ETA: 1s - loss: 0.6945 - f1: 0.45 - ETA: 1s - loss: 0.6945 - f1: 0.45 - ETA: 1s - loss: 0.6945 - f1: 0.45 - ETA: 1s - loss: 0.6942 - f1: 0.46 - ETA: 1s - loss: 0.6940 - f1: 0.47 - ETA: 1s - loss: 0.6940 - f1: 0.46 - ETA: 1s - loss: 0.6939 - f1: 0.47 - ETA: 1s - loss: 0.6939 - f1: 0.46 - ETA: 1s - loss: 0.6940 - f1: 0.46 - ETA: 1s - loss: 0.6940 - f1: 0.46 - ETA: 1s - loss: 0.6940 - f1: 0.46 - ETA: 1s - loss: 0.6940 - f1: 0.46 - ETA: 0s - loss: 0.6940 - f1: 0.46 - ETA: 0s - loss: 0.6941 - f1: 0.46 - ETA: 0s - loss: 0.6941 - f1: 0.46 - ETA: 0s - loss: 0.6942 - f1: 0.45 - ETA: 0s - loss: 0.6942 - f1: 0.45 - ETA: 0s - loss: 0.6941 - f1: 0.45 - ETA: 0s - loss: 0.6940 - f1: 0.45 - ETA: 0s - loss: 0.6940 - f1: 0.46 - ETA: 0s - loss: 0.6940 - f1: 0.46 - ETA: 0s - loss: 0.6940 - f1: 0.46 - ETA: 0s - loss: 0.6940 - f1: 0.46 - ETA: 0s - loss: 0.6939 - f1: 0.46 - ETA: 0s - loss: 0.6940 - f1: 0.46 - 5s 48ms/step - loss: 0.6940 - f1: 0.4643 - val_loss: 0.6930 - val_f1: 0.3287\nEpoch 7/10\n95/95 [==============================] - ETA: 7s - loss: 0.6946 - f1: 0.44 - ETA: 6s - loss: 0.6926 - f1: 0.54 - ETA: 4s - loss: 0.6930 - f1: 0.49 - ETA: 4s - loss: 0.6939 - f1: 0.49 - ETA: 4s - loss: 0.6941 - f1: 0.48 - ETA: 3s - loss: 0.6941 - f1: 0.41 - ETA: 3s - loss: 0.6939 - f1: 0.37 - ETA: 3s - loss: 0.6938 - f1: 0.38 - ETA: 3s - loss: 0.6939 - f1: 0.39 - ETA: 3s - loss: 0.6937 - f1: 0.42 - ETA: 3s - loss: 0.6937 - f1: 0.42 - ETA: 2s - loss: 0.6936 - f1: 0.43 - ETA: 2s - loss: 0.6937 - f1: 0.42 - ETA: 2s - loss: 0.6936 - f1: 0.42 - ETA: 2s - loss: 0.6937 - f1: 0.43 - ETA: 2s - loss: 0.6937 - f1: 0.43 - ETA: 2s - loss: 0.6936 - f1: 0.42 - ETA: 2s - loss: 0.6935 - f1: 0.43 - ETA: 2s - loss: 0.6934 - f1: 0.44 - ETA: 2s - loss: 0.6932 - f1: 0.45 - ETA: 2s - loss: 0.6932 - f1: 0.44 - ETA: 2s - loss: 0.6933 - f1: 0.45 - ETA: 1s - loss: 0.6933 - f1: 0.44 - ETA: 1s - loss: 0.6934 - f1: 0.44 - ETA: 1s - loss: 0.6934 - f1: 0.44 - ETA: 1s - loss: 0.6933 - f1: 0.45 - ETA: 1s - loss: 0.6934 - f1: 0.45 - ETA: 1s - loss: 0.6934 - f1: 0.45 - ETA: 1s - loss: 0.6934 - f1: 0.45 - ETA: 1s - loss: 0.6934 - f1: 0.45 - ETA: 1s - loss: 0.6934 - f1: 0.45 - ETA: 1s - loss: 0.6935 - f1: 0.44 - ETA: 1s - loss: 0.6935 - f1: 0.44 - ETA: 1s - loss: 0.6934 - f1: 0.45 - ETA: 1s - loss: 0.6934 - f1: 0.44 - ETA: 0s - loss: 0.6933 - f1: 0.45 - ETA: 0s - loss: 0.6933 - f1: 0.45 - ETA: 0s - loss: 0.6933 - f1: 0.45 - ETA: 0s - loss: 0.6932 - f1: 0.44 - ETA: 0s - loss: 0.6933 - f1: 0.44 - ETA: 0s - loss: 0.6932 - f1: 0.44 - ETA: 0s - loss: 0.6932 - f1: 0.44 - ETA: 0s - loss: 0.6932 - f1: 0.44 - ETA: 0s - loss: 0.6932 - f1: 0.44 - ETA: 0s - loss: 0.6931 - f1: 0.44 - ETA: 0s - loss: 0.6931 - f1: 0.44 - ETA: 0s - loss: 0.6931 - f1: 0.44 - ETA: 0s - loss: 0.6931 - f1: 0.44 - 5s 50ms/step - loss: 0.6931 - f1: 0.4464 - val_loss: 0.6922 - val_f1: 0.0227\nEpoch 8/10\n","name":"stdout"},{"output_type":"stream","text":"95/95 [==============================] - ETA: 8s - loss: 0.6908 - f1: 0.51 - ETA: 5s - loss: 0.6918 - f1: 0.54 - ETA: 4s - loss: 0.6918 - f1: 0.48 - ETA: 4s - loss: 0.6918 - f1: 0.46 - ETA: 3s - loss: 0.6922 - f1: 0.42 - ETA: 3s - loss: 0.6926 - f1: 0.39 - ETA: 3s - loss: 0.6926 - f1: 0.39 - ETA: 3s - loss: 0.6927 - f1: 0.38 - ETA: 3s - loss: 0.6929 - f1: 0.38 - ETA: 3s - loss: 0.6930 - f1: 0.40 - ETA: 3s - loss: 0.6930 - f1: 0.42 - ETA: 3s - loss: 0.6930 - f1: 0.41 - ETA: 3s - loss: 0.6928 - f1: 0.42 - ETA: 3s - loss: 0.6929 - f1: 0.41 - ETA: 2s - loss: 0.6928 - f1: 0.41 - ETA: 2s - loss: 0.6930 - f1: 0.41 - ETA: 2s - loss: 0.6929 - f1: 0.42 - ETA: 2s - loss: 0.6928 - f1: 0.42 - ETA: 2s - loss: 0.6927 - f1: 0.41 - ETA: 2s - loss: 0.6928 - f1: 0.42 - ETA: 2s - loss: 0.6928 - f1: 0.42 - ETA: 2s - loss: 0.6928 - f1: 0.42 - ETA: 2s - loss: 0.6928 - f1: 0.42 - ETA: 2s - loss: 0.6927 - f1: 0.42 - ETA: 2s - loss: 0.6927 - f1: 0.43 - ETA: 2s - loss: 0.6927 - f1: 0.43 - ETA: 2s - loss: 0.6928 - f1: 0.43 - ETA: 2s - loss: 0.6927 - f1: 0.43 - ETA: 1s - loss: 0.6928 - f1: 0.43 - ETA: 1s - loss: 0.6929 - f1: 0.44 - ETA: 1s - loss: 0.6930 - f1: 0.43 - ETA: 1s - loss: 0.6931 - f1: 0.43 - ETA: 1s - loss: 0.6932 - f1: 0.43 - ETA: 1s - loss: 0.6931 - f1: 0.43 - ETA: 1s - loss: 0.6931 - f1: 0.43 - ETA: 1s - loss: 0.6932 - f1: 0.43 - ETA: 1s - loss: 0.6932 - f1: 0.42 - ETA: 1s - loss: 0.6932 - f1: 0.42 - ETA: 1s - loss: 0.6932 - f1: 0.42 - ETA: 0s - loss: 0.6931 - f1: 0.43 - ETA: 0s - loss: 0.6931 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - ETA: 0s - loss: 0.6929 - f1: 0.42 - ETA: 0s - loss: 0.6929 - f1: 0.42 - ETA: 0s - loss: 0.6930 - f1: 0.42 - 5s 56ms/step - loss: 0.6930 - f1: 0.4248 - val_loss: 0.6914 - val_f1: 0.0000e+00\nEpoch 9/10\n95/95 [==============================] - ETA: 7s - loss: 0.6856 - f1: 0.59 - ETA: 4s - loss: 0.6892 - f1: 0.53 - ETA: 4s - loss: 0.6904 - f1: 0.45 - ETA: 4s - loss: 0.6904 - f1: 0.44 - ETA: 3s - loss: 0.6911 - f1: 0.41 - ETA: 3s - loss: 0.6906 - f1: 0.37 - ETA: 3s - loss: 0.6907 - f1: 0.34 - ETA: 3s - loss: 0.6914 - f1: 0.36 - ETA: 3s - loss: 0.6918 - f1: 0.38 - ETA: 3s - loss: 0.6917 - f1: 0.39 - ETA: 3s - loss: 0.6920 - f1: 0.38 - ETA: 2s - loss: 0.6922 - f1: 0.38 - ETA: 2s - loss: 0.6923 - f1: 0.36 - ETA: 2s - loss: 0.6924 - f1: 0.35 - ETA: 2s - loss: 0.6925 - f1: 0.35 - ETA: 2s - loss: 0.6926 - f1: 0.36 - ETA: 2s - loss: 0.6925 - f1: 0.36 - ETA: 2s - loss: 0.6927 - f1: 0.37 - ETA: 2s - loss: 0.6928 - f1: 0.37 - ETA: 2s - loss: 0.6928 - f1: 0.37 - ETA: 2s - loss: 0.6928 - f1: 0.37 - ETA: 2s - loss: 0.6928 - f1: 0.38 - ETA: 1s - loss: 0.6928 - f1: 0.38 - ETA: 1s - loss: 0.6927 - f1: 0.38 - ETA: 1s - loss: 0.6927 - f1: 0.38 - ETA: 1s - loss: 0.6928 - f1: 0.38 - ETA: 1s - loss: 0.6929 - f1: 0.39 - ETA: 1s - loss: 0.6929 - f1: 0.39 - ETA: 1s - loss: 0.6928 - f1: 0.39 - ETA: 1s - loss: 0.6927 - f1: 0.39 - ETA: 1s - loss: 0.6927 - f1: 0.39 - ETA: 1s - loss: 0.6926 - f1: 0.39 - ETA: 1s - loss: 0.6926 - f1: 0.39 - ETA: 1s - loss: 0.6927 - f1: 0.39 - ETA: 1s - loss: 0.6927 - f1: 0.39 - ETA: 0s - loss: 0.6928 - f1: 0.39 - ETA: 0s - loss: 0.6928 - f1: 0.39 - ETA: 0s - loss: 0.6927 - f1: 0.38 - ETA: 0s - loss: 0.6926 - f1: 0.38 - ETA: 0s - loss: 0.6926 - f1: 0.38 - ETA: 0s - loss: 0.6926 - f1: 0.38 - ETA: 0s - loss: 0.6926 - f1: 0.38 - ETA: 0s - loss: 0.6926 - f1: 0.38 - ETA: 0s - loss: 0.6925 - f1: 0.38 - ETA: 0s - loss: 0.6925 - f1: 0.38 - ETA: 0s - loss: 0.6926 - f1: 0.38 - ETA: 0s - loss: 0.6927 - f1: 0.38 - ETA: 0s - loss: 0.6927 - f1: 0.38 - 5s 55ms/step - loss: 0.6927 - f1: 0.3860 - val_loss: 0.6906 - val_f1: 0.0000e+00\nEpoch 10/10\n95/95 [==============================] - ETA: 7s - loss: 0.6922 - f1: 0.35 - ETA: 5s - loss: 0.6927 - f1: 0.39 - ETA: 4s - loss: 0.6916 - f1: 0.39 - ETA: 4s - loss: 0.6914 - f1: 0.39 - ETA: 3s - loss: 0.6911 - f1: 0.37 - ETA: 3s - loss: 0.6906 - f1: 0.32 - ETA: 3s - loss: 0.6899 - f1: 0.31 - ETA: 3s - loss: 0.6910 - f1: 0.32 - ETA: 3s - loss: 0.6917 - f1: 0.34 - ETA: 3s - loss: 0.6919 - f1: 0.35 - ETA: 2s - loss: 0.6918 - f1: 0.35 - ETA: 2s - loss: 0.6920 - f1: 0.36 - ETA: 2s - loss: 0.6919 - f1: 0.35 - ETA: 2s - loss: 0.6920 - f1: 0.35 - ETA: 2s - loss: 0.6919 - f1: 0.34 - ETA: 2s - loss: 0.6923 - f1: 0.35 - ETA: 2s - loss: 0.6921 - f1: 0.34 - ETA: 2s - loss: 0.6923 - f1: 0.34 - ETA: 2s - loss: 0.6923 - f1: 0.34 - ETA: 2s - loss: 0.6922 - f1: 0.35 - ETA: 2s - loss: 0.6923 - f1: 0.35 - ETA: 2s - loss: 0.6924 - f1: 0.35 - ETA: 1s - loss: 0.6923 - f1: 0.36 - ETA: 1s - loss: 0.6922 - f1: 0.35 - ETA: 1s - loss: 0.6923 - f1: 0.35 - ETA: 1s - loss: 0.6925 - f1: 0.35 - ETA: 1s - loss: 0.6925 - f1: 0.36 - ETA: 1s - loss: 0.6926 - f1: 0.36 - ETA: 1s - loss: 0.6926 - f1: 0.36 - ETA: 1s - loss: 0.6924 - f1: 0.36 - ETA: 1s - loss: 0.6925 - f1: 0.36 - ETA: 1s - loss: 0.6925 - f1: 0.35 - ETA: 1s - loss: 0.6924 - f1: 0.35 - ETA: 1s - loss: 0.6925 - f1: 0.35 - ETA: 0s - loss: 0.6925 - f1: 0.35 - ETA: 0s - loss: 0.6926 - f1: 0.35 - ETA: 0s - loss: 0.6924 - f1: 0.35 - ETA: 0s - loss: 0.6923 - f1: 0.35 - ETA: 0s - loss: 0.6922 - f1: 0.35 - ETA: 0s - loss: 0.6922 - f1: 0.35 - ETA: 0s - loss: 0.6922 - f1: 0.35 - ETA: 0s - loss: 0.6923 - f1: 0.35 - ETA: 0s - loss: 0.6923 - f1: 0.35 - ETA: 0s - loss: 0.6922 - f1: 0.34 - ETA: 0s - loss: 0.6922 - f1: 0.35 - ETA: 0s - loss: 0.6922 - f1: 0.35 - ETA: 0s - loss: 0.6923 - f1: 0.34 - 4s 47ms/step - loss: 0.6924 - f1: 0.3480 - val_loss: 0.6899 - val_f1: 0.0000e+00\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:cyan\"> |-Trial ID: fcdbb3743a34af286f3e333cdc94d121</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:cyan\"> |-Score: 0.5154328942298889</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:cyan\"> |-Best step: 0</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:cyan\"> |-num_filters: 64</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:blue\"> |-num_layers: 2</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:cyan\"> |-num_layers_rnn: 2</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:blue\"> |-optimizer: Adadelta</span>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<span style=\"color:cyan\"> |-units: 480</span>"},"metadata":{}},{"output_type":"stream","text":"Epoch 1/10\n95/95 [==============================]: 0.7048 - f1: 0.487 - 1s 371ms/step - loss: 0.6960 - f1: 0.628 - 1s 259ms/step - loss: 0.6981 - f1: 0.593 - 1s 204ms/step - loss: 0.7015 - f1: 0.538 - 1s 170ms/step - loss: 0.7004 - f1: 0.552 - 1s 147ms/step - loss: 0.6977 - f1: 0.591 - 1s 131ms/step - loss: 0.7003 - f1: 0.558 - 1s 119ms/step - loss: 0.6976 - f1: 0.588 - 1s 110ms/step - loss: 0.7008 - f1: 0.543 - 1s 102ms/step - loss: 0.7034 - f1: 0.501 - 1s 96ms/step - loss: 0.7050 - f1: 0.479 - 1s 91ms/step - loss: 0.7068 - f1: 0.45 - 1s 87ms/step - loss: 0.7079 - f1: 0.44 - 1s 83ms/step - loss: 0.7060 - f1: 0.46 - 1s 80ms/step - loss: 0.7037 - f1: 0.49 - 1s 77ms/step - loss: 0.7023 - f1: 0.51 - 1s 75ms/step - loss: 0.7009 - f1: 0.53 - 1s 73ms/step - loss: 0.7005 - f1: 0.53 - 1s 71ms/step - loss: 0.7003 - f1: 0.54 - 1s 69ms/step - loss: 0.7007 - f1: 0.53 - 1s 68ms/step - loss: 0.7013 - f1: 0.53 - 1s 66ms/step - loss: 0.7002 - f1: 0.54 - 1s 65ms/step - loss: 0.7002 - f1: 0.54 - 2s 64ms/step - loss: 0.7013 - f1: 0.53 - 2s 63ms/step - loss: 0.7014 - f1: 0.53 - 2s 62ms/step - loss: 0.7018 - f1: 0.52 - 2s 61ms/step - loss: 0.7012 - f1: 0.53 - 2s 60ms/step - loss: 0.7008 - f1: 0.54 - 2s 59ms/step - loss: 0.7012 - f1: 0.53 - 2s 58ms/step - loss: 0.7009 - f1: 0.54 - 2s 58ms/step - loss: 0.6998 - f1: 0.55 - 2s 57ms/step - loss: 0.7002 - f1: 0.54 - 2s 56ms/step - loss: 0.7007 - f1: 0.54 - 2s 56ms/step - loss: 0.7006 - f1: 0.54 - 2s 55ms/step - loss: 0.7002 - f1: 0.54 - 2s 55ms/step - loss: 0.6996 - f1: 0.55 - 2s 54ms/step - loss: 0.6996 - f1: 0.55 - 2s 54ms/step - loss: 0.6990 - f1: 0.56 - 2s 53ms/step - loss: 0.6994 - f1: 0.55 - 2s 53ms/step - loss: 0.6994 - f1: 0.56 - 2s 53ms/step - loss: 0.6991 - f1: 0.56 - 2s 52ms/step - loss: 0.6983 - f1: 0.57 - 2s 52ms/step - loss: 0.6988 - f1: 0.56 - 2s 52ms/step - loss: 0.6988 - f1: 0.56 - 2s 51ms/step - loss: 0.6986 - f1: 0.57 - 2s 51ms/step - loss: 0.6987 - f1: 0.56 - 2s 51ms/step - loss: 0.6990 - f1: 0.56 - 2s 50ms/step - loss: 0.6988 - f1: 0.56 - 2s 50ms/step - loss: 0.6988 - f1: 0.56 - 2s 50ms/step - loss: 0.6984 - f1: 0.57 - 3s 50ms/step - loss: 0.6981 - f1: 0.57 - 3s 49ms/step - loss: 0.6980 - f1: 0.57 - 3s 49ms/step - loss: 0.6977 - f1: 0.57 - 3s 49ms/step - loss: 0.6978 - f1: 0.57 - 3s 49ms/step - loss: 0.6978 - f1: 0.57 - 3s 49ms/step - loss: 0.6976 - f1: 0.58 - 3s 48ms/step - loss: 0.6977 - f1: 0.58 - 3s 48ms/step - loss: 0.6980 - f1: 0.57 - 3s 48ms/step - loss: 0.6981 - f1: 0.57 - 3s 48ms/step - loss: 0.6982 - f1: 0.57 - 3s 47ms/step - loss: 0.6981 - f1: 0.57 - 3s 47ms/step - loss: 0.6984 - f1: 0.57 - 3s 47ms/step - loss: 0.6985 - f1: 0.57 - 3s 47ms/step - loss: 0.6982 - f1: 0.57 - 3s 47ms/step - loss: 0.6986 - f1: 0.57 - 3s 47ms/step - loss: 0.6979 - f1: 0.57 - 3s 46ms/step - loss: 0.6981 - f1: 0.57 - 3s 46ms/step - loss: 0.6984 - f1: 0.57 - 3s 46ms/step - loss: 0.6983 - f1: 0.57 - 3s 46ms/step - loss: 0.6980 - f1: 0.57 - 3s 46ms/step - loss: 0.6980 - f1: 0.57 - 3s 46ms/step - loss: 0.6978 - f1: 0.57 - 3s 46ms/step - loss: 0.6981 - f1: 0.57 - 3s 45ms/step - loss: 0.6982 - f1: 0.57 - 3s 45ms/step - loss: 0.6986 - f1: 0.56 - 3s 45ms/step - loss: 0.6985 - f1: 0.57 - 3s 45ms/step - loss: 0.6987 - f1: 0.56 - 4s 45ms/step - loss: 0.6987 - f1: 0.56 - 4s 45ms/step - loss: 0.6989 - f1: 0.56 - 4s 45ms/step - loss: 0.6985 - f1: 0.56 - 4s 45ms/step - loss: 0.6988 - f1: 0.56 - 4s 45ms/step - loss: 0.6987 - f1: 0.56 - 4s 45ms/step - loss: 0.6985 - f1: 0.56 - 4s 45ms/step - loss: 0.6986 - f1: 0.56 - 4s 45ms/step - loss: 0.6985 - f1: 0.56 - 4s 45ms/step - loss: 0.6987 - f1: 0.56 - 4s 45ms/step - loss: 0.6988 - f1: 0.56 - 4s 45ms/step - loss: 0.6988 - f1: 0.56 - 4s 45ms/step - loss: 0.6986 - f1: 0.56 - 4s 45ms/step - loss: 0.6986 - f1: 0.56 - 4s 45ms/step - loss: 0.6986 - f1: 0.56 - 4s 45ms/step - loss: 0.6982 - f1: 0.57 - 4s 44ms/step - loss: 0.6984 - f1: 0.57 - 4s 44ms/step - loss: 0.6982 - f1: 0.57 - 4s 44ms/step - loss: 0.6982 - f1: 0.57 - 6s 62ms/step - loss: 0.6982 - f1: 0.5730 - val_loss: 0.7035 - val_f1: 0.5154\nEpoch 2/10\n95/95 [==============================] - ETA: 6s - loss: 0.7025 - f1: 0.51 - ETA: 4s - loss: 0.6967 - f1: 0.60 - ETA: 3s - loss: 0.7005 - f1: 0.55 - ETA: 3s - loss: 0.6996 - f1: 0.56 - ETA: 3s - loss: 0.7001 - f1: 0.53 - ETA: 3s - loss: 0.7043 - f1: 0.47 - ETA: 3s - loss: 0.7062 - f1: 0.44 - ETA: 2s - loss: 0.7025 - f1: 0.48 - ETA: 2s - loss: 0.7000 - f1: 0.52 - ETA: 2s - loss: 0.6996 - f1: 0.53 - ETA: 2s - loss: 0.7004 - f1: 0.52 - ETA: 2s - loss: 0.6994 - f1: 0.54 - ETA: 2s - loss: 0.7001 - f1: 0.53 - ETA: 2s - loss: 0.6998 - f1: 0.53 - ETA: 2s - loss: 0.7001 - f1: 0.53 - ETA: 2s - loss: 0.6988 - f1: 0.54 - ETA: 2s - loss: 0.6996 - f1: 0.53 - ETA: 2s - loss: 0.6991 - f1: 0.54 - ETA: 2s - loss: 0.6984 - f1: 0.55 - ETA: 1s - loss: 0.6985 - f1: 0.55 - ETA: 1s - loss: 0.6982 - f1: 0.55 - ETA: 1s - loss: 0.6980 - f1: 0.55 - ETA: 1s - loss: 0.6978 - f1: 0.56 - ETA: 1s - loss: 0.6983 - f1: 0.55 - ETA: 1s - loss: 0.6981 - f1: 0.56 - ETA: 1s - loss: 0.6973 - f1: 0.57 - ETA: 1s - loss: 0.6970 - f1: 0.57 - ETA: 1s - loss: 0.6970 - f1: 0.57 - ETA: 1s - loss: 0.6970 - f1: 0.57 - ETA: 1s - loss: 0.6975 - f1: 0.57 - ETA: 1s - loss: 0.6976 - f1: 0.57 - ETA: 1s - loss: 0.6981 - f1: 0.56 - ETA: 1s - loss: 0.6980 - f1: 0.56 - ETA: 1s - loss: 0.6976 - f1: 0.56 - ETA: 0s - loss: 0.6978 - f1: 0.56 - ETA: 0s - loss: 0.6976 - f1: 0.57 - ETA: 0s - loss: 0.6977 - f1: 0.57 - ETA: 0s - loss: 0.6982 - f1: 0.56 - ETA: 0s - loss: 0.6984 - f1: 0.56 - ETA: 0s - loss: 0.6986 - f1: 0.55 - ETA: 0s - loss: 0.6984 - f1: 0.56 - ETA: 0s - loss: 0.6981 - f1: 0.56 - ETA: 0s - loss: 0.6981 - f1: 0.56 - ETA: 0s - loss: 0.6984 - f1: 0.55 - ETA: 0s - loss: 0.6983 - f1: 0.56 - ETA: 0s - loss: 0.6984 - f1: 0.56 - ETA: 0s - loss: 0.6981 - f1: 0.56 - 4s 43ms/step - loss: 0.6980 - f1: 0.5675 - val_loss: 0.7029 - val_f1: 0.5154\nEpoch 3/10\n93/95 [============================>.] - ETA: 6s - loss: 0.7006 - f1: 0.55 - ETA: 4s - loss: 0.6941 - f1: 0.62 - ETA: 3s - loss: 0.6987 - f1: 0.57 - ETA: 3s - loss: 0.6990 - f1: 0.56 - ETA: 3s - loss: 0.7000 - f1: 0.54 - ETA: 3s - loss: 0.7032 - f1: 0.48 - ETA: 2s - loss: 0.7057 - f1: 0.44 - ETA: 2s - loss: 0.7022 - f1: 0.49 - ETA: 2s - loss: 0.6999 - f1: 0.52 - ETA: 2s - loss: 0.6995 - f1: 0.53 - ETA: 2s - loss: 0.7004 - f1: 0.52 - ETA: 2s - loss: 0.6995 - f1: 0.54 - ETA: 2s - loss: 0.7005 - f1: 0.53 - ETA: 2s - loss: 0.7002 - f1: 0.53 - ETA: 2s - loss: 0.7003 - f1: 0.53 - ETA: 2s - loss: 0.6990 - f1: 0.54 - ETA: 2s - loss: 0.6999 - f1: 0.53 - ETA: 2s - loss: 0.6994 - f1: 0.54 - ETA: 1s - loss: 0.6987 - f1: 0.55 - ETA: 1s - loss: 0.6986 - f1: 0.55 - ETA: 1s - loss: 0.6986 - f1: 0.55 - ETA: 1s - loss: 0.6983 - f1: 0.55 - ETA: 1s - loss: 0.6980 - f1: 0.56 - ETA: 1s - loss: 0.6984 - f1: 0.55 - ETA: 1s - loss: 0.6984 - f1: 0.56 - ETA: 1s - loss: 0.6976 - f1: 0.57 - ETA: 1s - loss: 0.6973 - f1: 0.57 - ETA: 1s - loss: 0.6974 - f1: 0.57 - ETA: 1s - loss: 0.6973 - f1: 0.58 - ETA: 1s - loss: 0.6977 - f1: 0.57 - ETA: 1s - loss: 0.6977 - f1: 0.57 - ETA: 1s - loss: 0.6982 - f1: 0.57 - ETA: 1s - loss: 0.6983 - f1: 0.57 - ETA: 0s - loss: 0.6978 - f1: 0.57 - ETA: 0s - loss: 0.6980 - f1: 0.57 - ETA: 0s - loss: 0.6977 - f1: 0.57 - ETA: 0s - loss: 0.6978 - f1: 0.57 - ETA: 0s - loss: 0.6981 - f1: 0.56 - ETA: 0s - loss: 0.6983 - f1: 0.56 - ETA: 0s - loss: 0.6984 - f1: 0.56 - ETA: 0s - loss: 0.6983 - f1: 0.56 - ETA: 0s - loss: 0.6980 - f1: 0.56 - ETA: 0s - loss: 0.6980 - f1: 0.56 - ETA: 0s - loss: 0.6983 - f1: 0.56 - ETA: 0s - loss: 0.6981 - f1: 0.56 - ETA: 0s - loss: 0.6982 - f1: 0.56 - ETA: 0s - loss: 0.6981 - f1: 0.5680","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tuner.results_summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models = tuner.get_best_models(num_models=1)\n\nmodels[0].summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"models[0].predict(test_dataset)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result_dataframe = pd.DataFrame(columns=['id', 'target'])\nresult_dataframe['id'] = test_df['id']\nresult_dataframe['target'] = np.where(np.array(models[0].predict(test_dataset)) >= 0.5, 1, 0 )\nresult_dataframe.to_csv('result.csv', index= False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}