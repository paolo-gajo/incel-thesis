{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel, BertPreTrainedModel, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from pgfuncs import tokenize_and_vectorize, pad_trunc, collect_expected, tokenize_and_vectorize_1dlist, collect_expected_1dlist, df_classification_report\n",
    "\n",
    "from datetime import datetime\n",
    "# timestamp for file naming\n",
    "now = datetime.now()\n",
    "time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "date_str = now.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # used to make train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load incelsis_5203 dataset\n",
    "df_incelsis_5203 = pd.read_csv('/home/pgajo/working/data/datasets/English/Incels.is/IFD-EN-5203_splits.csv')\n",
    "\n",
    "df_train_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'train_incelsis']\n",
    "df_dev_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'dev_incelsis']\n",
    "df_test_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'test_incelsis']\n",
    "\n",
    "# Print the size of each split\n",
    "print('Incels.is train set size:', len(df_train_incelsis_5203))\n",
    "print('Incels.is dev set size:', len(df_dev_incelsis_5203))\n",
    "print('Incels.is test set size:', len(df_test_incelsis_5203))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fdb_250 dataset\n",
    "df_fdb_250 = pd.read_csv('/home/pgajo/working/data/datasets/Italian/Il_forum_dei_brutti/IFD-IT-250.csv')\n",
    "df_fdb_250 = df_fdb_250[['hs','text']]\n",
    "df_fdb_250\n",
    "df_fdb_250['data_type']='test_fdb_250'\n",
    "\n",
    "print('Forum dei brutti test set size:', len(df_fdb_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the davidson set\n",
    "file_path_csv_davidson = '/home/pgajo/working/data/datasets/English/hate-speech-and-offensive-language (davidson)/davidson_labeled_data.csv'\n",
    "df_davidson = pd.read_csv(file_path_csv_davidson, index_col=None)\n",
    "df_davidson = df_davidson[['hs','text']]\n",
    "df_davidson['data_type']='davidson'\n",
    "df_davidson = df_davidson.sample(frac=1).reset_index(drop=True) # shuffle the set\n",
    "mask = df_davidson['hs'] >= 1\n",
    "\n",
    "# Set those values to 1\n",
    "df_davidson.loc[mask, 'hs'] = 1\n",
    "\n",
    "# Split the data into training and test sets (70% for training, 30% for test)\n",
    "df_train_davidson, df_test_davidson = train_test_split(df_davidson, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_davidson, df_test_davidson = train_test_split(df_test_davidson, test_size=0.5, random_state=42)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize=True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_davidson[df_train_davidson['hs'] == 1].sample(n=num_hs_1, replace=True)\n",
    "df_hs_0 = df_train_davidson[df_train_davidson['hs'] == 0].sample(n=num_hs_0, replace=True)\n",
    "df_train_davidson_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=True))\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=False))\n",
    "\n",
    "# Print the sample\n",
    "print('df_train_davidson_sample value_counts:')\n",
    "print(df_train_davidson_sample['hs'].value_counts(normalize=False))\n",
    "print()\n",
    "\n",
    "# Print the size of each split\n",
    "df_train_davidson['data_type']='train_davidson'\n",
    "df_dev_davidson['data_type']='dev_davidson'\n",
    "df_test_davidson['data_type']='test_davidson'\n",
    "print('Davidson full train set size:', len(df_train_davidson))\n",
    "print('Davidson full dev set size:', len(df_dev_davidson))\n",
    "print('Davidson full test set size:', len(df_test_davidson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hateval_2019_english set\n",
    "file_path_csv_hateval_2019_english_train = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_train_miso.csv'\n",
    "file_path_csv_hateval_2019_english_dev = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_dev_miso.csv'\n",
    "file_path_csv_hateval_2019_english_test = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_test_miso.csv'\n",
    "\n",
    "df_train_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_train, index_col = None)\n",
    "df_dev_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_dev, index_col = None)\n",
    "df_test_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_test, index_col = None)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_english_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval english sample value_counts:')\n",
    "print(df_train_hateval_2019_english_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_english['data_type']='train_hateval_2019_english'\n",
    "df_dev_hateval_2019_english['data_type']='dev_hateval_2019_english'\n",
    "df_test_hateval_2019_english['data_type']='test_hateval_2019_english'\n",
    "print('HatEval english full train set size:', len(df_train_hateval_2019_english))\n",
    "print('HatEval english full dev set size:', len(df_dev_hateval_2019_english))\n",
    "print('HatEval english full test set size:', len(df_test_hateval_2019_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hateval_2019_spanish set\n",
    "file_path_csv_hateval_2019_spanish_train = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_train.csv'\n",
    "file_path_csv_hateval_2019_spanish_dev = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_dev.csv'\n",
    "file_path_csv_hateval_2019_spanish_test = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_test.csv'\n",
    "\n",
    "df_train_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_train, index_col = None)\n",
    "df_train_hateval_2019_spanish = df_train_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_dev_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_dev, index_col = None)\n",
    "df_dev_hateval_2019_spanish = df_dev_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_test_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_test, index_col = None)\n",
    "df_test_hateval_2019_spanish = df_test_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_spanish_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval spanish sample value_counts:')\n",
    "print(df_train_hateval_2019_spanish_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_spanish['data_type']='train_hateval_2019_spanish'\n",
    "df_dev_hateval_2019_spanish['data_type']='dev_hateval_2019_spanish'\n",
    "df_test_hateval_2019_spanish['data_type']='test_hateval_2019_spanish'\n",
    "print('HatEval spanish full train set size:', len(df_train_hateval_2019_spanish))\n",
    "print('HatEval spanish full dev set size:', len(df_dev_hateval_2019_spanish))\n",
    "print('HatEval spanish full test set size:', len(df_test_hateval_2019_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HateXplain dataset\n",
    "import json\n",
    "filename_json = '/home/pgajo/working/data/datasets/English/HateXplain/Data/dataset.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(filename_json, 'r') as f:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "def post_majority_vote_choice(label_list):\n",
    "    '''\n",
    "    Returns the majority vote for a post in the HateXplain json dataset.\n",
    "    '''\n",
    "    label_dict={}\n",
    "    for i,post_label in enumerate(label_list):\n",
    "        # print(i,post_label)\n",
    "        if post_label not in label_dict:\n",
    "            label_dict[post_label]=1\n",
    "        else:\n",
    "            label_dict[post_label]+=1\n",
    "    max_key = max(label_dict, key=label_dict.get)\n",
    "    if label_dict[max_key]>1:\n",
    "        return max_key # return the label key with the highest value if > 1\n",
    "\n",
    "df_hatexplain_list = []\n",
    "for key_post in dataset_json.keys():\n",
    "    post = []\n",
    "    labels_post = [key_annotators['label'] for key_annotators in dataset_json[key_post]['annotators']] # get the list of labels\n",
    "    label_majority=post_majority_vote_choice(labels_post) # return the majority label\n",
    "    if label_majority!=None: # the post_majority_vote_choice returns None if there is no majority label, i.e., they all have the same occurrences\n",
    "        post.append(label_majority) # append the label of the post\n",
    "        post.append(' '.join(dataset_json[key_post]['post_tokens'])) # append the text tokens of the post\n",
    "        df_hatexplain_list.append(post) # append the label-text pair\n",
    "df_hatexplain=pd.DataFrame(df_hatexplain_list, columns=['hs','text'])\n",
    "df_hatexplain_binary = df_hatexplain.loc[df_hatexplain['hs'] != 'offensive']\n",
    "df_hatexplain_binary['hs'] = df_hatexplain_binary['hs'].replace({'normal': 0, 'hatespeech': 1})\n",
    "# df_hatexplain_binary\n",
    "# Split the data into training and test sets (80% for training, 20% for test)\n",
    "hatexplain_binary_devtest_size=0.2\n",
    "df_train_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_hatexplain_binary, test_size=hatexplain_binary_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_test_hatexplain_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_hatexplain_binary['data_type']='hatexplain_binary_train'\n",
    "df_dev_hatexplain_binary['data_type']='hatexplain_binary_dev'\n",
    "df_test_hatexplain_binary['data_type']='hatexplain_binary_test'\n",
    "print('HateXplain binary dev+test split ratio:',hatexplain_binary_devtest_size)\n",
    "print('HateXplain binary full train set size:', len(df_train_hatexplain_binary))\n",
    "print('HateXplain binary full dev set size:', len(df_dev_hatexplain_binary))\n",
    "print('HateXplain binary full test set size:', len(df_test_hatexplain_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stormfront dataset from \"Hate speech dataset from a white supremacist forum\"\n",
    "\n",
    "df_stormfront_raw=pd.read_csv('/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/annotations_metadata.csv')\n",
    "df_stormfront_raw['label'] = df_stormfront_raw['label'].replace({'noHate': 0, 'hate': 1})\n",
    "df_stormfront_raw = df_stormfront_raw.rename(columns={'label': 'hs'})\n",
    "\n",
    "post_dir='/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/all_files'\n",
    "dict_ids_labels={}\n",
    "dict_post_pairs_ws=[]\n",
    "\n",
    "for row in df_stormfront_raw.values.tolist():\n",
    "    dict_ids_labels[row[0]]=row[4]\n",
    "len(dict_ids_labels)\n",
    "for filename in os.listdir(post_dir):\n",
    "    with open(os.path.join(post_dir, filename), 'r') as file:\n",
    "        # Read the contents of the file into a string variable\n",
    "        file_contents = file.read()\n",
    "        filename=filename[:-4]\n",
    "    dict_post_pairs_ws.append([dict_ids_labels[filename],file_contents,filename])\n",
    "df_stormfront=pd.DataFrame(dict_post_pairs_ws, columns=['hs','text','filename'])\n",
    "df_stormfront = df_stormfront[(df_stormfront['hs'] == 0) | (df_stormfront['hs'] == 1)]\n",
    "df_stormfront['hs']=df_stormfront['hs'].astype(int)\n",
    "\n",
    "# Split the data into training and test sets (80% for training, 30% for test)\n",
    "df_stormfront_devtest_size=0.3\n",
    "df_train_stormfront, df_test_stormfront = train_test_split(df_stormfront, test_size=df_stormfront_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_stormfront, df_test_stormfront = train_test_split(df_test_stormfront, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_stormfront['data_type']='df_stormfront_train'\n",
    "df_dev_stormfront['data_type']='df_stormfront_dev'\n",
    "df_test_stormfront['data_type']='df_stormfront_test'\n",
    "print('Stormfront dataset dev+test split size:',df_stormfront_devtest_size)\n",
    "print('Stormfront dataset train set size:', len(df_train_stormfront))\n",
    "print('Stormfront dataset dev set size:', len(df_dev_stormfront))\n",
    "print('Stormfront dataset test set size:', len(df_test_stormfront))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evalita18twitter set\n",
    "file_path_csv_evalita18twitter_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/TW-folder-20230313T173228Z-001/TW-folder/TW-train/haspeede_TW-train.tsv'\n",
    "\n",
    "df_train_evalita18twitter = pd.read_csv(file_path_csv_evalita18twitter_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18twitter.columns=['id','text','hs']\n",
    "# display(df_train_evalita18twitter)\n",
    "df_train_evalita18twitter['data_type'] = 'train_evalita18twitter'\n",
    "print('evalita18twitter full train set size:', len(df_train_evalita18twitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evalita18facebook set\n",
    "file_path_csv_evalita18facebook_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/FB-folder-20230313T173818Z-001/FB-folder/FB-train/haspeede_FB-train.tsv'\n",
    "\n",
    "df_train_evalita18facebook = pd.read_csv(file_path_csv_evalita18facebook_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18facebook['data_type'] = 'train_evalita18facebook'\n",
    "# display(df_train_evalita18facebook)\n",
    "print('evalita18facebook full train set size:', len(df_train_evalita18facebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evalita20 set\n",
    "file_path_csv_evalita20_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2020/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "df_train_evalita20 = pd.read_csv(file_path_csv_evalita20_train, sep='\\t', index_col = None)\n",
    "# display(df_train_evalita20)\n",
    "\n",
    "print('evalita20 full train set size:', len(df_train_evalita20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the offenseval_2020 dataset\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# configs = ['ar', 'da', 'en', 'gr', 'tr']\n",
    "# datasets = {}\n",
    "\n",
    "# for config in configs:\n",
    "#     datasets[config] = load_dataset(\"strombergnlp/offenseval_2020\", config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_id=0\n",
    "device_index = 1 # set to -1 for multigpu # Set the index of the CUDA device you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "\n",
    "# set problem type\n",
    "prob_type = 'binary'\n",
    "\n",
    "# set task name\n",
    "task_name = 'incelsis'\n",
    "\n",
    "# define dataset combinations\n",
    "metrics_list_names=[\n",
    "    # monolingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_incelsis_5203'], # 0\n",
    "    ['train_incelsis_5203+train_davidson_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 1\n",
    "    ['train_incelsis_5203+train_hateval_2019_english_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 2\n",
    "    ['train_incelsis_5203+train_davidson_sample+train_hateval_2019_english_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 3\n",
    "    ['train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 4\n",
    "    ['train_hateval_2019_english+train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'], # 5\n",
    "    ['train_incelsis_5203', 'dev_hateval_2019_english', 'test_hateval_2019_english'], # 6\n",
    "    ['train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'], # 7\n",
    "    ['train_incelsis_5203', 'dev_davidson', 'test_davidson'], # 8\n",
    "    ['train_incelsis_5203+train_davidson+train_hateval_2019_english', 'dev_davidson', 'test_davidson'], # 9\n",
    "    ['train_incelsis_5203+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 10\n",
    "    ['train_hatexplain_binary', 'hatexplain_binary_dev', 'hatexplain_binary_test'], # 11\n",
    "    ['train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'], # 12\n",
    "    ['train_incelsis_5203+train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'], # 13\n",
    "    ['train_incelsis_5203+train_hatexplain_binary+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 14\n",
    "    ['train_incelsis_5203+train_stormfront', 'dev_incelsis_5203', 'test_incelsis_5203'], # 15\n",
    "    ['train_incelsis_5203+train_stormfront+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 16\n",
    "\n",
    "    # multilingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 17\n",
    "    ['train_incelsis_5203+train_hateval_2019_english', 'dev_incelsis_5203', 'test_fdb_250'], # 18\n",
    "    ['train_incelsis_5203+train_hateval_2019_spanish', 'dev_incelsis_5203', 'test_fdb_250'], # 19\n",
    "    ['train_incelsis_5203+train_hateval_2019_english+train_hateval_2019_spanish', 'dev_incelsis_5203', 'test_fdb_250'], # 20\n",
    "    ['train_incelsis_5203+train_evalita18facebook', 'dev_incelsis_5203', 'test_fdb_250'], # 21\n",
    "    ['train_incelsis_5203+train_evalita18twitter', 'dev_incelsis_5203', 'test_fdb_250'], # 22\n",
    "    ['train_incelsis_5203+train_evalita18facebook+train_evalita18twitter', 'dev_incelsis_5203', 'test_fdb_250'], # 23\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 24\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 25\n",
    "\n",
    "]\n",
    "\n",
    "# set train datasets\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "if 'incelsis' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_incelsis_5203])\n",
    "\n",
    "if 'davidson' in metrics_list_names[metrics_id][0]:\n",
    "    if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "        df_train = pd.concat([df_train,df_train_davidson_sample])\n",
    "    else:\n",
    "        df_train = pd.concat([df_train,df_train_davidson])\n",
    "\n",
    "if 'hateval' in metrics_list_names[metrics_id][0]:\n",
    "    if 'english' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_spanish])\n",
    "\n",
    "if 'train_hatexplain_binary' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_hatexplain_binary])\n",
    "\n",
    "if 'train_stormfront' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_stormfront])\n",
    "\n",
    "if 'train_evalita18facebook' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_evalita18facebook])\n",
    "\n",
    "if 'train_evalita18twitter' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_evalita18twitter])\n",
    "\n",
    "df_dev = pd.DataFrame()\n",
    "# set dev datasets\n",
    "if 'dev_incelsis_5203' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_incelsis_5203])\n",
    "\n",
    "if 'dev_davidson' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_davidson])\n",
    "\n",
    "if 'dev_hateval_2019' in metrics_list_names[metrics_id][1]:\n",
    "    if 'english' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev,df_dev_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev,df_dev_hateval_2019_spanish])\n",
    "\n",
    "if 'dev_hatexplain_binary' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_hatexplain_binary])\n",
    "\n",
    "if 'dev_stormfront' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_stormfront])\n",
    "\n",
    "# set test datasets\n",
    "if 'test_incelsis_5203' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_incelsis_5203\n",
    "\n",
    "if 'test_davidson' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_davidson\n",
    "\n",
    "if 'test_hateval_2019' in metrics_list_names[metrics_id][2]:\n",
    "    if 'english' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_english\n",
    "    if 'spanish' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_spanish\n",
    "\n",
    "if 'test_hatexplain_binary' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_hatexplain_binary\n",
    "\n",
    "if 'test_stormfront' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_stormfront\n",
    "\n",
    "if 'test_fdb_250' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_fdb_250\n",
    "\n",
    "df_train = df_train.sample(frac = 1)\n",
    "df_dev= df_dev.sample(frac = 1)\n",
    "\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_dev), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification, BertConfig, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# monolingual models\n",
    "# model_name = 'bert-base-uncased'\n",
    "# model_name = '/home/pgajo/working/pt_models/HateBERT'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-100k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-1M'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert'\n",
    "# model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "# model_name = 'roberta-base'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-roberta-base-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-roberta-base-100k'\n",
    "\n",
    "# multilingual models\n",
    "model_name = 'bert-base-multilingual-cased'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-100k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-1M'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model_name_simple=model_name.split('/')[-1]\n",
    "# print(model.eval())\n",
    "# print(model.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training data using the tokenizer\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_train.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encode the validation data using the tokenizer\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_dev.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encode the validation data using the tokenizer\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_test.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract IDs, attention masks and labels from training dataset\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.hs.values)\n",
    "# Extract IDs, attention masks and labels from validation dataset\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df_dev.hs.values)\n",
    "# Extract IDs, attention masks and labels from test dataset\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(df_test.hs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation dataset from extracted features\n",
    "from torch.utils.data import TensorDataset\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "print(\"Train set length: {}\\nDev set length: {}\\nTest set length: {}\".format(len(dataset_train), len(dataset_val), len(dataset_test)))\n",
    "\n",
    "# Define the size of each batch\n",
    "batch_size = 16  # number of examples to include in each batch\n",
    "\n",
    "# Load training dataset\n",
    "dataloader_train= DataLoader(\n",
    "    dataset_train,  # training dataset to load\n",
    "    sampler=RandomSampler(dataset_train),  # randomly sample examples from the training dataset\n",
    "    batch_size=batch_size  # set the batch size to the defined value\n",
    ")\n",
    "\n",
    "# Load valuation dataset\n",
    "dataloader_val= DataLoader(\n",
    "    dataset_val,  # valuation dataset to load\n",
    "    sampler=RandomSampler(dataset_val),  # randomly sample examples from the valuation dataset\n",
    "    batch_size=batch_size  # set the batch size to the defined value\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "dataloader_test= DataLoader(\n",
    "    dataset_test,  # testuation dataset to load\n",
    "    sampler=RandomSampler(dataset_test),  # randomly sample examples from the valuation dataset\n",
    "    batch_size=batch_size  # set the batch size to the defined value\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4  # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model optimizer -> Adam\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),  # optimize the parameters of the model\n",
    "    lr = 1e-5,  # set the learning rate to 1e-5\n",
    "    eps = 1e-8  # set the epsilon value to 1e-8\n",
    ")\n",
    "\n",
    "# Define model scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  # the optimizer to use\n",
    "                                            num_warmup_steps=0,  # number of warmup steps\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)  # number of total training steps\n",
    "\n",
    "# Define random seeds\n",
    "seed_val = 17  # set the seed value to 17\n",
    "\n",
    "# Set the seed value for the random number generators in different modules\n",
    "random.seed(seed_val)  # set the seed value for the random module's random number generator\n",
    "np.random.seed(seed_val)  # set the seed value for NumPy's random number generator\n",
    "torch.manual_seed(seed_val)  # set the seed value for PyTorch's CPU random number generator\n",
    "torch.cuda.manual_seed_all(seed_val)  # set the seed value for PyTorch's GPU random number generators (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup (1,2,both)\n",
    "\n",
    "# Set the device\n",
    "if device_index in [0,1]:\n",
    "    device = torch.device(f\"cuda:{device_index}\")\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    multi_gpu=0\n",
    "else:\n",
    "    device = torch.device(f\"cuda\")\n",
    "    model.to(device)\n",
    "    from torch.nn import DataParallel\n",
    "    model = DataParallel(model)\n",
    "    multi_gpu=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the F1 score computed on the predictions\n",
    "def f1_score_func(preds, labels, problem_type):\n",
    "    # print('preds',preds)\n",
    "    # print('len(preds)',len(preds))\n",
    "    # print('labels',labels)\n",
    "    # print('len(labels)',len(labels))\n",
    "\n",
    "    if problem_type == 'binary':\n",
    "        average_metric = 'binary'\n",
    "        # preds_flat = np.round(preds).flatten()\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    elif problem_type == 'multiclass':\n",
    "        average_metric = 'macro'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid problem_type argument. Use either \"binary\" or \"multiclass\".')\n",
    "    labels_flat = labels.flatten()\n",
    "\n",
    "    # print('preds_flat',preds_flat)\n",
    "    # print('len(preds_flat)',len(preds_flat))\n",
    "\n",
    "    # print('labels_flat',labels_flat)\n",
    "    # print('len(labels_flat)',len(labels_flat))\n",
    "\n",
    "    \n",
    "\n",
    "    return f1_score(labels_flat, preds_flat, average=average_metric)\n",
    "\n",
    "# Returns the precision computed on the predictions\n",
    "def prec_func(preds, labels, problem_type):\n",
    "    if problem_type == 'binary':\n",
    "        average_metric = 'binary'\n",
    "        # preds_flat = np.round(preds).flatten()\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    elif problem_type == 'multiclass':\n",
    "        average_metric = 'macro'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid problem_type argument. Use either \"binary\" or \"multiclass\".')\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    return precision_score(labels_flat, preds_flat, average=average_metric)\n",
    "\n",
    "# Returns the recall computed on the predictions\n",
    "def recall_func(preds, labels, problem_type):\n",
    "    if problem_type == 'binary':\n",
    "        average_metric = 'binary'\n",
    "        # preds_flat = np.round(preds).flatten()\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    elif problem_type == 'multiclass':\n",
    "        average_metric = 'macro'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid problem_type argument. Use either \"binary\" or \"multiclass\".')\n",
    "    \n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average=average_metric)\n",
    "\n",
    "# Evaluates the model using the validation set\n",
    "def evaluate(dataloader_val,setting='',multi_gpu=0):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "            # 'labels': batch[2].unsqueeze(1).float()\n",
    "                                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        if multi_gpu: # do the mean of the two losses if i'm using 2 GPUs\n",
    "            loss=loss.mean()\n",
    "            \n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item() # loss.mean() when training with multiple gpus, multiple batches at a time, giving multiple losses at a time\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    # print('predictions[0]',predictions[0])\n",
    "    # print('len(predictions[0])',len(predictions[0]))\n",
    "    # print(len(predictions[0])>2)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    if len(predictions[0])>2 and setting=='binary':\n",
    "        # print('len(predictions[0]>2)')\n",
    "        # print(predictions[0])\n",
    "        predictions=predictions[:,:len(predictions[0])-1]\n",
    "        # predictions=np.argmax(predictions[:,:len(predictions[0])-1], axis=1)\n",
    "        # print('np.argmax(predictions[:,:len(predictions[0])-1], axis=1) --> predictions[0]: ',predictions[0])\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename bits\n",
    "multilingual=0\n",
    "if multilingual:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_multilingual'\n",
    "else:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_monolingual'\n",
    "\n",
    "metrics_save_path_model = os.path.join(metrics_save_path, model_name_simple)\n",
    "\n",
    "if not os.path.exists(metrics_save_path_model):\n",
    "    os.mkdir(metrics_save_path_model)\n",
    "\n",
    "metrics_filename = str(metrics_id)+'_'+model_name_simple+'_'+time_str+'_metrics.csv'\n",
    "metrics_csv_filepath = os.path.join(metrics_save_path_model, metrics_filename)\n",
    "print(metrics_csv_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "# write set identifiers for the pandas metrics dataframe\n",
    "df_metrics_train_set_string=''\n",
    "for i,index in enumerate(df_train['data_type'].value_counts(normalize = False).index.to_list()):\n",
    "    set_len=df_train['data_type'].value_counts(normalize = False).values[i]\n",
    "    df_metrics_train_set_string+=index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "df_metrics_dev_set_string=''\n",
    "for i,index in enumerate(df_dev['data_type'].value_counts(normalize = False).index.to_list()):\n",
    "    set_len=df_dev['data_type'].value_counts(normalize = False).values[i]\n",
    "    df_metrics_dev_set_string+=index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "df_metrics_test_set_string=''\n",
    "for i,index in enumerate(df_test['data_type'].value_counts(normalize = False).index.to_list()):\n",
    "    set_len=df_test['data_type'].value_counts(normalize = False).values[i]\n",
    "    df_metrics_test_set_string+=index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "# train\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize = False))\n",
    "print('Dev set length:', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize = False))\n",
    "print('Test set length:', len(df_test), '\\n')\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'dev_loss', 'dev_f1', 'dev_precision', 'dev_recall', 'test_loss', 'test_f1', 'test_precision', 'test_recall'])\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # set the model in training mode\n",
    "\n",
    "    loss_train_total = 0  # initialize the total training loss\n",
    "\n",
    "    # Create a progress bar for the training dataloader\n",
    "    progress_bar = tqdm(dataloader_train,\n",
    "                        desc=model_name_simple+' - Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                        disable=False\n",
    "                        )\n",
    "\n",
    "    # Loop over the batches in the training dataloader\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()  # reset the gradients to 0 for each batch\n",
    "        batch = tuple(b.to(device) for b in batch)  # move the batch to the device (e.g. GPU)\n",
    "        inputs = {\n",
    "                'input_ids': batch[0],  # input_ids are the token ids\n",
    "                'attention_mask': batch[1],  # attention_mask masks the padding tokens\n",
    "                'labels': batch[2]\n",
    "                }  # the true labels of the input\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]  # the first element of outputs is the loss\n",
    "\n",
    "        if multi_gpu:\n",
    "            loss=loss.mean()\n",
    "\n",
    "        loss_train_total += loss.item()  # accumulate the training loss\n",
    "\n",
    "        loss.backward()  # backpropagate the loss through the model to compute gradients\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the gradients to prevent explosion\n",
    "\n",
    "        optimizer.step()  # update the model parameters using the computed gradients\n",
    "        scheduler.step()  # update the learning rate using the learning rate scheduler\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})  # update the progress bar to show the current loss\n",
    "    \n",
    "    # Print the epoch number and number of total epochs\n",
    "    # tqdm.write(f'\\nEpoch {epoch}/{epochs}')\n",
    "\n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "    # tqdm.write(f'Training loss: {loss_train_avg}')  # make sure that model is still training\n",
    "\n",
    "    dev_loss, pred_val, true_values_val = evaluate(dataloader_val, prob_type, multi_gpu)  # to check overtraining (or overfitting)\n",
    "    dev_f1 = f1_score_func(pred_val, true_values_val, prob_type)\n",
    "    dev_prec = prec_func(pred_val, true_values_val, prob_type)\n",
    "    dev_recall = recall_func(pred_val, true_values_val, prob_type)\n",
    "\n",
    "    # tqdm.write(f'Validation loss: {dev_loss}')\n",
    "    # tqdm.write(f'F1 Score ({prob_type}): {dev_f1}')\n",
    "    # tqdm.write(f'Precision Score ({prob_type}): {dev_prec}')\n",
    "    # tqdm.write(f'Recall Score ({prob_type}): {dev_recall}')\n",
    "\n",
    "    test_loss, pred_test, true_values_test = evaluate(dataloader_test, prob_type, multi_gpu)  # to check overtraining (or overfitting)\n",
    "    test_f1 = f1_score_func(pred_test, true_values_test, prob_type)\n",
    "    test_prec = prec_func(pred_test, true_values_test, prob_type)\n",
    "    test_recall = recall_func(pred_test, true_values_test, prob_type)\n",
    "\n",
    "    # tqdm.write(f'Test loss: {test_loss}')\n",
    "    # tqdm.write(f'F1 Score ({prob_type}): {test_f1}')\n",
    "    # tqdm.write(f'Precision Score ({prob_type}): {test_prec}')\n",
    "    # tqdm.write(f'Recall Score ({prob_type}): {test_recall}')\n",
    "\n",
    "    df_metrics = df_metrics.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': loss_train_avg,\n",
    "        'dev_loss': dev_loss,\n",
    "        'dev_f1': dev_f1,\n",
    "        'dev_precision': dev_prec,\n",
    "        'dev_recall': dev_recall,\n",
    "        'test_loss': test_loss,\n",
    "        'test_f1': test_f1,\n",
    "        'test_precision': test_prec,\n",
    "        'test_recall': test_recall\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    df_metrics['model']=model_name_simple\n",
    "    df_metrics['train_len']=str(len(df_train))\n",
    "    df_metrics['train_set(s)']=df_metrics_train_set_string[:-1]\n",
    "    df_metrics['dev_set(s)']=df_metrics_dev_set_string[:-1]\n",
    "    df_metrics['test_set(s)']=df_metrics_test_set_string[:-1]\n",
    "    df_metrics['run_id']=metrics_id\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print('Run ID:', metrics_id)\n",
    "    print('Train sets:')\n",
    "    print(df_train['data_type'].value_counts(normalize = False))\n",
    "    print('Train set length:', len(df_train), '\\n')\n",
    "    print('Dev sets:')\n",
    "    print(df_dev['data_type'].value_counts(normalize = False))\n",
    "    print('Dev set length:', len(df_dev), '\\n')\n",
    "    print('Test sets:')\n",
    "    print(df_test['data_type'].value_counts(normalize = False))\n",
    "    print('Test set length:', len(df_test), '\\n')\n",
    "    \n",
    "    display(df_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_csv_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_metrics.to_csv(metrics_csv_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgajo-Fz_qUQZq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca09a575e78e41b281752c78b59ffb95d2982f5008f73e33df6571d672f258d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
