{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 20:46:32.195343: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 20:46:32.305977: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-27 20:46:32.305996: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-27 20:46:32.872444: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-27 20:46:32.872508: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-27 20:46:32.872514: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel, BertPreTrainedModel, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from pgfuncs import tokenize_and_vectorize, pad_trunc, collect_expected, tokenize_and_vectorize_1dlist, collect_expected_1dlist, df_classification_report\n",
    "\n",
    "from datetime import datetime\n",
    "# timestamp for file naming\n",
    "now = datetime.now()\n",
    "time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "date_str = now.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # used to make train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incels.is train set size: 3642\n",
      "Incels.is dev set size: 780\n",
      "Incels.is test set size: 781\n"
     ]
    }
   ],
   "source": [
    "# load incelsis_5203 dataset\n",
    "df_incelsis_5203 = pd.read_csv('/home/pgajo/working/data/datasets/English/Incels.is/IFD-EN-5203_splits.csv')\n",
    "\n",
    "df_train_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'train_incelsis']\n",
    "df_dev_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'dev_incelsis']\n",
    "df_test_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'test_incelsis']\n",
    "\n",
    "# Print the size of each split\n",
    "print('Incels.is train set size:', len(df_train_incelsis_5203))\n",
    "print('Incels.is dev set size:', len(df_dev_incelsis_5203))\n",
    "print('Incels.is test set size:', len(df_test_incelsis_5203))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forum dei brutti test set size: 250\n"
     ]
    }
   ],
   "source": [
    "# load fdb_250 dataset\n",
    "df_fdb_250 = pd.read_csv('/home/pgajo/working/data/datasets/Italian/Il_forum_dei_brutti/IFD-IT-250.csv')\n",
    "df_fdb_250 = df_fdb_250[['hs','text']]\n",
    "df_fdb_250\n",
    "df_fdb_250['data_type']='test_fdb_250'\n",
    "\n",
    "print('Forum dei brutti test set size:', len(df_fdb_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_davidson_sample value_counts:\n",
      "0    2160\n",
      "1    1482\n",
      "Name: hs, dtype: int64\n",
      "\n",
      "Davidson full train set size: 17348\n",
      "Davidson full dev set size: 3717\n",
      "Davidson full test set size: 3718\n"
     ]
    }
   ],
   "source": [
    "# load the davidson set\n",
    "file_path_csv_davidson = '/home/pgajo/working/data/datasets/English/hate-speech-and-offensive-language (davidson)/davidson_labeled_data.csv'\n",
    "df_davidson = pd.read_csv(file_path_csv_davidson, index_col=None)\n",
    "df_davidson = df_davidson[['hs','text']]\n",
    "df_davidson['data_type']='davidson'\n",
    "df_davidson = df_davidson.sample(frac=1).reset_index(drop=True) # shuffle the set\n",
    "mask = df_davidson['hs'] >= 1\n",
    "\n",
    "# Set those values to 1\n",
    "df_davidson.loc[mask, 'hs'] = 1\n",
    "\n",
    "# Split the data into training and test sets (70% for training, 30% for test)\n",
    "df_train_davidson, df_test_davidson = train_test_split(df_davidson, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_davidson, df_test_davidson = train_test_split(df_test_davidson, test_size=0.5, random_state=42)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize=True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_davidson[df_train_davidson['hs'] == 1].sample(n=num_hs_1, replace=True)\n",
    "df_hs_0 = df_train_davidson[df_train_davidson['hs'] == 0].sample(n=num_hs_0, replace=True)\n",
    "df_train_davidson_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=True))\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=False))\n",
    "\n",
    "# Print the sample\n",
    "print('df_train_davidson_sample value_counts:')\n",
    "print(df_train_davidson_sample['hs'].value_counts(normalize=False))\n",
    "print()\n",
    "\n",
    "# Print the size of each split\n",
    "df_train_davidson['data_type']='train_davidson'\n",
    "df_dev_davidson['data_type']='dev_davidson'\n",
    "df_test_davidson['data_type']='test_davidson'\n",
    "print('Davidson full train set size:', len(df_train_davidson))\n",
    "print('Davidson full dev set size:', len(df_dev_davidson))\n",
    "print('Davidson full test set size:', len(df_test_davidson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HatEval english sample value_counts:\n",
      "0    2160\n",
      "1    1482\n",
      "Name: hs, dtype: int64\n",
      "\n",
      "HatEval english full train set size: 4500\n",
      "HatEval english full dev set size: 500\n",
      "HatEval english full test set size: 1500\n"
     ]
    }
   ],
   "source": [
    "# load the hateval_2019_english set\n",
    "file_path_csv_hateval_2019_english_train = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_train_miso.csv'\n",
    "file_path_csv_hateval_2019_english_dev = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_dev_miso.csv'\n",
    "file_path_csv_hateval_2019_english_test = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_test_miso.csv'\n",
    "\n",
    "df_train_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_train, index_col = None)\n",
    "df_dev_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_dev, index_col = None)\n",
    "df_test_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_test, index_col = None)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_english_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval english sample value_counts:')\n",
    "print(df_train_hateval_2019_english_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_english['data_type']='train_hateval_2019_english'\n",
    "df_dev_hateval_2019_english['data_type']='dev_hateval_2019_english'\n",
    "df_test_hateval_2019_english['data_type']='test_hateval_2019_english'\n",
    "print('HatEval english full train set size:', len(df_train_hateval_2019_english))\n",
    "print('HatEval english full dev set size:', len(df_dev_hateval_2019_english))\n",
    "print('HatEval english full test set size:', len(df_test_hateval_2019_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HatEval spanish sample value_counts:\n",
      "0    2160\n",
      "1    1482\n",
      "Name: hs, dtype: int64\n",
      "\n",
      "HatEval spanish full train set size: 4500\n",
      "HatEval spanish full dev set size: 500\n",
      "HatEval spanish full test set size: 1600\n"
     ]
    }
   ],
   "source": [
    "# load the hateval_2019_spanish set\n",
    "file_path_csv_hateval_2019_spanish_train = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_train.csv'\n",
    "file_path_csv_hateval_2019_spanish_dev = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_dev.csv'\n",
    "file_path_csv_hateval_2019_spanish_test = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_test.csv'\n",
    "\n",
    "df_train_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_train, index_col = None)\n",
    "df_train_hateval_2019_spanish = df_train_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_dev_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_dev, index_col = None)\n",
    "df_dev_hateval_2019_spanish = df_dev_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_test_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_test, index_col = None)\n",
    "df_test_hateval_2019_spanish = df_test_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_spanish_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval spanish sample value_counts:')\n",
    "print(df_train_hateval_2019_spanish_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_spanish['data_type']='train_hateval_2019_spanish'\n",
    "df_dev_hateval_2019_spanish['data_type']='dev_hateval_2019_spanish'\n",
    "df_test_hateval_2019_spanish['data_type']='test_hateval_2019_spanish'\n",
    "print('HatEval spanish full train set size:', len(df_train_hateval_2019_spanish))\n",
    "print('HatEval spanish full dev set size:', len(df_dev_hateval_2019_spanish))\n",
    "print('HatEval spanish full test set size:', len(df_test_hateval_2019_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HateXplain binary dev+test split ratio: 0.2\n",
      "HateXplain binary full train set size: 10999\n",
      "HateXplain binary full dev set size: 1375\n",
      "HateXplain binary full test set size: 1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2389239/361674652.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hatexplain_binary['hs'] = df_hatexplain_binary['hs'].replace({'normal': 0, 'hatespeech': 1})\n"
     ]
    }
   ],
   "source": [
    "# load the HateXplain dataset\n",
    "import json\n",
    "filename_json = '/home/pgajo/working/data/datasets/English/HateXplain/Data/dataset.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(filename_json, 'r') as f:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "def post_majority_vote_choice(label_list):\n",
    "    '''\n",
    "    Returns the majority vote for a post in the HateXplain json dataset.\n",
    "    '''\n",
    "    label_dict={}\n",
    "    for i,post_label in enumerate(label_list):\n",
    "        # print(i,post_label)\n",
    "        if post_label not in label_dict:\n",
    "            label_dict[post_label]=1\n",
    "        else:\n",
    "            label_dict[post_label]+=1\n",
    "    max_key = max(label_dict, key=label_dict.get)\n",
    "    if label_dict[max_key]>1:\n",
    "        return max_key # return the label key with the highest value if > 1\n",
    "\n",
    "df_hatexplain_list = []\n",
    "for key_post in dataset_json.keys():\n",
    "    post = []\n",
    "    labels_post = [key_annotators['label'] for key_annotators in dataset_json[key_post]['annotators']] # get the list of labels\n",
    "    label_majority=post_majority_vote_choice(labels_post) # return the majority label\n",
    "    if label_majority!=None: # the post_majority_vote_choice returns None if there is no majority label, i.e., they all have the same occurrences\n",
    "        post.append(label_majority) # append the label of the post\n",
    "        post.append(' '.join(dataset_json[key_post]['post_tokens'])) # append the text tokens of the post\n",
    "        df_hatexplain_list.append(post) # append the label-text pair\n",
    "df_hatexplain=pd.DataFrame(df_hatexplain_list, columns=['hs','text'])\n",
    "df_hatexplain_binary = df_hatexplain.loc[df_hatexplain['hs'] != 'offensive']\n",
    "df_hatexplain_binary['hs'] = df_hatexplain_binary['hs'].replace({'normal': 0, 'hatespeech': 1})\n",
    "# df_hatexplain_binary\n",
    "# Split the data into training and test sets (80% for training, 20% for test)\n",
    "hatexplain_binary_devtest_size=0.2\n",
    "df_train_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_hatexplain_binary, test_size=hatexplain_binary_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_test_hatexplain_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_hatexplain_binary['data_type']='hatexplain_binary_train'\n",
    "df_dev_hatexplain_binary['data_type']='hatexplain_binary_dev'\n",
    "df_test_hatexplain_binary['data_type']='hatexplain_binary_test'\n",
    "print('HateXplain binary dev+test split ratio:',hatexplain_binary_devtest_size)\n",
    "print('HateXplain binary full train set size:', len(df_train_hatexplain_binary))\n",
    "print('HateXplain binary full dev set size:', len(df_dev_hatexplain_binary))\n",
    "print('HateXplain binary full test set size:', len(df_test_hatexplain_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stormfront dataset dev+test split size: 0.3\n",
      "Stormfront dataset train set size: 7492\n",
      "Stormfront dataset dev set size: 1605\n",
      "Stormfront dataset test set size: 1606\n"
     ]
    }
   ],
   "source": [
    "# load the stormfront dataset from \"Hate speech dataset from a white supremacist forum\"\n",
    "\n",
    "df_stormfront_raw=pd.read_csv('/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/annotations_metadata.csv')\n",
    "df_stormfront_raw['label'] = df_stormfront_raw['label'].replace({'noHate': 0, 'hate': 1})\n",
    "df_stormfront_raw = df_stormfront_raw.rename(columns={'label': 'hs'})\n",
    "\n",
    "post_dir='/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/all_files'\n",
    "dict_ids_labels={}\n",
    "dict_post_pairs_ws=[]\n",
    "\n",
    "for row in df_stormfront_raw.values.tolist():\n",
    "    dict_ids_labels[row[0]]=row[4]\n",
    "len(dict_ids_labels)\n",
    "for filename in os.listdir(post_dir):\n",
    "    with open(os.path.join(post_dir, filename), 'r') as file:\n",
    "        # Read the contents of the file into a string variable\n",
    "        file_contents = file.read()\n",
    "        filename=filename[:-4]\n",
    "    dict_post_pairs_ws.append([dict_ids_labels[filename],file_contents,filename])\n",
    "df_stormfront=pd.DataFrame(dict_post_pairs_ws, columns=['hs','text','filename'])\n",
    "df_stormfront = df_stormfront[(df_stormfront['hs'] == 0) | (df_stormfront['hs'] == 1)]\n",
    "df_stormfront['hs']=df_stormfront['hs'].astype(int)\n",
    "\n",
    "# Split the data into training and test sets (80% for training, 30% for test)\n",
    "df_stormfront_devtest_size=0.3\n",
    "df_train_stormfront, df_test_stormfront = train_test_split(df_stormfront, test_size=df_stormfront_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_stormfront, df_test_stormfront = train_test_split(df_test_stormfront, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_stormfront['data_type']='df_stormfront_train'\n",
    "df_dev_stormfront['data_type']='df_stormfront_dev'\n",
    "df_test_stormfront['data_type']='df_stormfront_test'\n",
    "print('Stormfront dataset dev+test split size:',df_stormfront_devtest_size)\n",
    "print('Stormfront dataset train set size:', len(df_train_stormfront))\n",
    "print('Stormfront dataset dev set size:', len(df_dev_stormfront))\n",
    "print('Stormfront dataset test set size:', len(df_test_stormfront))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalita18twitter full train set size: 3000\n"
     ]
    }
   ],
   "source": [
    "# load the evalita18twitter set\n",
    "file_path_csv_evalita18twitter_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/TW-folder-20230313T173228Z-001/TW-folder/TW-train/haspeede_TW-train.tsv'\n",
    "\n",
    "df_train_evalita18twitter = pd.read_csv(file_path_csv_evalita18twitter_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18twitter.columns=['id','text','hs']\n",
    "# display(df_train_evalita18twitter)\n",
    "df_train_evalita18twitter['data_type'] = 'train_evalita18twitter'\n",
    "print('evalita18twitter full train set size:', len(df_train_evalita18twitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalita18facebook full train set size: 3000\n"
     ]
    }
   ],
   "source": [
    "# load the evalita18facebook set\n",
    "file_path_csv_evalita18facebook_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/FB-folder-20230313T173818Z-001/FB-folder/FB-train/haspeede_FB-train.tsv'\n",
    "\n",
    "df_train_evalita18facebook = pd.read_csv(file_path_csv_evalita18facebook_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18facebook['data_type'] = 'train_evalita18facebook'\n",
    "# display(df_train_evalita18facebook)\n",
    "print('evalita18facebook full train set size:', len(df_train_evalita18facebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalita20 full train set size: 6837\n"
     ]
    }
   ],
   "source": [
    "# load the evalita20 set\n",
    "file_path_csv_evalita20_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2020/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "df_train_evalita20 = pd.read_csv(file_path_csv_evalita20_train, sep='\\t', index_col = None)\n",
    "# display(df_train_evalita20)\n",
    "\n",
    "print('evalita20 full train set size:', len(df_train_evalita20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the offenseval_2020 dataset\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# configs = ['ar', 'da', 'en', 'gr', 'tr']\n",
    "# datasets = {}\n",
    "\n",
    "# for config in configs:\n",
    "#     datasets[config] = load_dataset(\"strombergnlp/offenseval_2020\", config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_id=0\n",
    "device_index = -1 # set to -1 for multigpu # Set the index of the CUDA device you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 0\n",
      "Train sets:\n",
      "train_incelsis    3642\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 3642 \n",
      "\n",
      "Dev sets:\n",
      "dev_incelsis    780\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 780 \n",
      "\n",
      "Test sets:\n",
      "test_incelsis    781\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 780 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# experiment setup\n",
    "\n",
    "# set problem type\n",
    "prob_type = 'binary'\n",
    "\n",
    "# set task name\n",
    "task_name = 'incelsis'\n",
    "\n",
    "# define dataset combinations\n",
    "metrics_list_names=[\n",
    "    # monolingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_incelsis_5203'], # 0\n",
    "    ['train_incelsis_5203+train_davidson_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 1\n",
    "    ['train_incelsis_5203+train_hateval_2019_english_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 2\n",
    "    ['train_incelsis_5203+train_davidson_sample+train_hateval_2019_english_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 3\n",
    "    ['train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 4\n",
    "    ['train_hateval_2019_english+train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'], # 5\n",
    "    ['train_incelsis_5203', 'dev_hateval_2019_english', 'test_hateval_2019_english'], # 6\n",
    "    ['train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'], # 7\n",
    "    ['train_incelsis_5203', 'dev_davidson', 'test_davidson'], # 8\n",
    "    ['train_incelsis_5203+train_davidson+train_hateval_2019_english', 'dev_davidson', 'test_davidson'], # 9\n",
    "    ['train_incelsis_5203+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 10\n",
    "    ['train_hatexplain_binary', 'hatexplain_binary_dev', 'hatexplain_binary_test'], # 11\n",
    "    ['train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'], # 12\n",
    "    ['train_incelsis_5203+train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'], # 13\n",
    "    ['train_incelsis_5203+train_hatexplain_binary+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 14\n",
    "    ['train_incelsis_5203+train_stormfront', 'dev_incelsis_5203', 'test_incelsis_5203'], # 15\n",
    "    ['train_incelsis_5203+train_stormfront+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 16\n",
    "\n",
    "    # multilingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 17\n",
    "    ['train_incelsis_5203+train_hateval_2019_english', 'dev_incelsis_5203', 'test_fdb_250'], # 18\n",
    "    ['train_incelsis_5203+train_hateval_2019_spanish', 'dev_incelsis_5203', 'test_fdb_250'], # 19\n",
    "    ['train_incelsis_5203+train_hateval_2019_english+train_hateval_2019_spanish', 'dev_incelsis_5203', 'test_fdb_250'], # 20\n",
    "    ['train_incelsis_5203+train_evalita18facebook', 'dev_incelsis_5203', 'test_fdb_250'], # 21\n",
    "    ['train_incelsis_5203+train_evalita18twitter', 'dev_incelsis_5203', 'test_fdb_250'], # 22\n",
    "    ['train_incelsis_5203+train_evalita18facebook+train_evalita18twitter', 'dev_incelsis_5203', 'test_fdb_250'], # 23\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 24\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 25\n",
    "\n",
    "]\n",
    "\n",
    "# set train datasets\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "if 'incelsis' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_incelsis_5203])\n",
    "\n",
    "if 'davidson' in metrics_list_names[metrics_id][0]:\n",
    "    if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "        df_train = pd.concat([df_train,df_train_davidson_sample])\n",
    "    else:\n",
    "        df_train = pd.concat([df_train,df_train_davidson])\n",
    "\n",
    "if 'hateval' in metrics_list_names[metrics_id][0]:\n",
    "    if 'english' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_spanish])\n",
    "\n",
    "if 'train_hatexplain_binary' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_hatexplain_binary])\n",
    "\n",
    "if 'train_stormfront' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_stormfront])\n",
    "\n",
    "if 'train_evalita18facebook' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_evalita18facebook])\n",
    "\n",
    "if 'train_evalita18twitter' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_evalita18twitter])\n",
    "\n",
    "df_dev = pd.DataFrame()\n",
    "# set dev datasets\n",
    "if 'dev_incelsis_5203' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_incelsis_5203])\n",
    "\n",
    "if 'dev_davidson' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_davidson])\n",
    "\n",
    "if 'dev_hateval_2019' in metrics_list_names[metrics_id][1]:\n",
    "    if 'english' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev,df_dev_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev,df_dev_hateval_2019_spanish])\n",
    "\n",
    "if 'dev_hatexplain_binary' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_hatexplain_binary])\n",
    "\n",
    "if 'dev_stormfront' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_stormfront])\n",
    "\n",
    "# set test datasets\n",
    "if 'test_incelsis_5203' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_incelsis_5203\n",
    "\n",
    "if 'test_davidson' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_davidson\n",
    "\n",
    "if 'test_hateval_2019' in metrics_list_names[metrics_id][2]:\n",
    "    if 'english' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_english\n",
    "    if 'spanish' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_spanish\n",
    "\n",
    "if 'test_hatexplain_binary' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_hatexplain_binary\n",
    "\n",
    "if 'test_stormfront' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_stormfront\n",
    "\n",
    "if 'test_fdb_250' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_fdb_250\n",
    "\n",
    "df_train = df_train.sample(frac = 1)\n",
    "df_dev= df_dev.sample(frac = 1)\n",
    "\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_dev), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification, BertConfig, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# monolingual models\n",
    "# model_name = 'bert-base-uncased'\n",
    "# model_name = '/home/pgajo/working/pt_models/HateBERT'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-100k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-1M'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert'\n",
    "model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "# model_name = 'roberta-base'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-roberta-base-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-roberta-base-100k'\n",
    "\n",
    "# multilingual models\n",
    "# model_name = 'bert-base-multilingual-cased'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-100k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-1M'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model_name_simple=model_name.split('/')[-1]\n",
    "# print(model.eval())\n",
    "# print(model.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2499773a5fc048c8b9aa733e4e989fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pgajo/.local/share/virtualenvs/pgajo-Fz_qUQZq/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e1b4b85fae49aa8a40800a346f23bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "289e54acf14745aba029a73956d9e5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/781 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Encode the training data using the tokenizer\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_train.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encode the validation data using the tokenizer\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_dev.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encode the validation data using the tokenizer\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_test.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract IDs, attention masks and labels from training dataset\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.hs.values)\n",
    "# Extract IDs, attention masks and labels from validation dataset\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df_dev.hs.values)\n",
    "# Extract IDs, attention masks and labels from test dataset\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(df_test.hs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set length: 3642\n",
      "Dev set length: 780\n",
      "Test set length: 781\n"
     ]
    }
   ],
   "source": [
    "# Create train and validation dataset from extracted features\n",
    "from torch.utils.data import TensorDataset\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "print(\"Train set length: {}\\nDev set length: {}\\nTest set length: {}\".format(len(dataset_train), len(dataset_val), len(dataset_test)))\n",
    "\n",
    "# Define the size of each batch\n",
    "batch_size = 16  # number of examples to include in each batch\n",
    "\n",
    "# Load training dataset\n",
    "dataloader_train= DataLoader(\n",
    "    dataset_train,  # training dataset to load\n",
    "    sampler=RandomSampler(dataset_train),  # randomly sample examples from the training dataset\n",
    "    batch_size=batch_size  # set the batch size to the defined value\n",
    ")\n",
    "\n",
    "# Load valuation dataset\n",
    "dataloader_val= DataLoader(\n",
    "    dataset_val,  # valuation dataset to load\n",
    "    sampler=RandomSampler(dataset_val),  # randomly sample examples from the valuation dataset\n",
    "    batch_size=batch_size  # set the batch size to the defined value\n",
    ")\n",
    "\n",
    "# Load test dataset\n",
    "dataloader_test= DataLoader(\n",
    "    dataset_test,  # testuation dataset to load\n",
    "    sampler=RandomSampler(dataset_test),  # randomly sample examples from the valuation dataset\n",
    "    batch_size=batch_size  # set the batch size to the defined value\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4  # number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pgajo/.local/share/virtualenvs/pgajo-Fz_qUQZq/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define model optimizer -> Adam\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),  # optimize the parameters of the model\n",
    "    lr = 1e-5,  # set the learning rate to 1e-5\n",
    "    eps = 1e-8  # set the epsilon value to 1e-8\n",
    ")\n",
    "\n",
    "# Define model scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,  # the optimizer to use\n",
    "                                            num_warmup_steps=0,  # number of warmup steps\n",
    "                                            num_training_steps=len(dataloader_train)*epochs)  # number of total training steps\n",
    "\n",
    "# Define random seeds\n",
    "seed_val = 17  # set the seed value to 17\n",
    "\n",
    "# Set the seed value for the random number generators in different modules\n",
    "random.seed(seed_val)  # set the seed value for the random module's random number generator\n",
    "np.random.seed(seed_val)  # set the seed value for NumPy's random number generator\n",
    "torch.manual_seed(seed_val)  # set the seed value for PyTorch's CPU random number generator\n",
    "torch.cuda.manual_seed_all(seed_val)  # set the seed value for PyTorch's GPU random number generators (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU setup (1,2,both)\n",
    "\n",
    "# Set the device\n",
    "if device_index in [0,1]:\n",
    "    device = torch.device(f\"cuda:{device_index}\")\n",
    "    model.to(device)\n",
    "    print(device)\n",
    "    multi_gpu=0\n",
    "else:\n",
    "    device = torch.device(f\"cuda\")\n",
    "    model.to(device)\n",
    "    from torch.nn import DataParallel\n",
    "    model = DataParallel(model)\n",
    "    multi_gpu=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the F1 score computed on the predictions\n",
    "def f1_score_func(preds, labels, problem_type):\n",
    "    if problem_type == 'binary':\n",
    "        average_metric = 'binary'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    elif problem_type == 'multiclass':\n",
    "        average_metric = 'macro'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid problem_type argument. Use either \"binary\" or \"multiclass\".')\n",
    "    labels_flat = labels.flatten()\n",
    "    return f1_score(labels_flat, preds_flat, average=average_metric)\n",
    "\n",
    "# Returns the precision computed on the predictions\n",
    "def prec_func(preds, labels, problem_type):\n",
    "    if problem_type == 'binary':\n",
    "        average_metric = 'binary'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    elif problem_type == 'multiclass':\n",
    "        average_metric = 'macro'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid problem_type argument. Use either \"binary\" or \"multiclass\".')\n",
    "    labels_flat = labels.flatten()\n",
    "    return precision_score(labels_flat, preds_flat, average=average_metric)\n",
    "\n",
    "# Returns the recall computed on the predictions\n",
    "def recall_func(preds, labels, problem_type):\n",
    "    if problem_type == 'binary':\n",
    "        average_metric = 'binary'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    elif problem_type == 'multiclass':\n",
    "        average_metric = 'macro'\n",
    "        preds_flat = np.argmax(preds, axis=1).flatten()\n",
    "    else:\n",
    "        raise ValueError('Invalid problem_type argument. Use either \"binary\" or \"multiclass\".')\n",
    "    labels_flat = labels.flatten()\n",
    "    return recall_score(labels_flat, preds_flat, average=average_metric)\n",
    "\n",
    "# Evaluates the model using the validation set\n",
    "def evaluate(dataloader_val,setting='',multi_gpu=0):\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals = [], []\n",
    "\n",
    "    for batch in tqdm(dataloader_val):\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "                                  }\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        loss = outputs[0]\n",
    "        if multi_gpu: # do the mean of the two losses if i'm using 2 GPUs\n",
    "            loss=loss.mean()\n",
    "            \n",
    "        logits = outputs[1]\n",
    "        loss_val_total += loss.item() # loss.mean() when training with multiple gpus, multiple batches at a time, giving multiple losses at a time\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = inputs['labels'].cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "\n",
    "    loss_val_avg = loss_val_total / len(dataloader_val)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    if len(predictions[0])>2 and setting=='binary':\n",
    "        predictions=predictions[:,:len(predictions[0])-1]\n",
    "    return loss_val_avg, predictions, true_vals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/data/metrics/metrics_monolingual/bert-base-uncased-hatexplain/0_bert-base-uncased-hatexplain_2023-03-27_20-46-34_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# filename bits\n",
    "multilingual=0\n",
    "if multilingual:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_multilingual'\n",
    "else:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_monolingual'\n",
    "\n",
    "metrics_save_path_model = os.path.join(metrics_save_path, model_name_simple)\n",
    "\n",
    "if not os.path.exists(metrics_save_path_model):\n",
    "    os.mkdir(metrics_save_path_model)\n",
    "\n",
    "metrics_filename = str(metrics_id)+'_'+model_name_simple+'_'+time_str+'_metrics.csv'\n",
    "metrics_csv_filepath = os.path.join(metrics_save_path_model, metrics_filename)\n",
    "print(metrics_csv_filepath)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 0\n",
      "Train sets:\n",
      "train_incelsis    3642\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 3642 \n",
      "\n",
      "Dev sets:\n",
      "dev_incelsis    780\n",
      "Name: data_type, dtype: int64\n",
      "Dev set length: 780 \n",
      "\n",
      "Test sets:\n",
      "test_incelsis    781\n",
      "Name: data_type, dtype: int64\n",
      "Test set length: 781 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>dev_loss</th>\n",
       "      <th>dev_f1</th>\n",
       "      <th>dev_precision</th>\n",
       "      <th>dev_recall</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>model</th>\n",
       "      <th>train_len</th>\n",
       "      <th>train_set(s)</th>\n",
       "      <th>dev_set(s)</th>\n",
       "      <th>test_set(s)</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.590648</td>\n",
       "      <td>0.305677</td>\n",
       "      <td>0.839344</td>\n",
       "      <td>0.870748</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.313553</td>\n",
       "      <td>0.812057</td>\n",
       "      <td>0.841912</td>\n",
       "      <td>0.784247</td>\n",
       "      <td>bert-base-uncased-hatexplain</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.261547</td>\n",
       "      <td>0.286005</td>\n",
       "      <td>0.859843</td>\n",
       "      <td>0.855799</td>\n",
       "      <td>0.863924</td>\n",
       "      <td>0.281830</td>\n",
       "      <td>0.825127</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.832192</td>\n",
       "      <td>bert-base-uncased-hatexplain</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.180992</td>\n",
       "      <td>0.301318</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.866883</td>\n",
       "      <td>0.844937</td>\n",
       "      <td>0.304679</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.859155</td>\n",
       "      <td>0.835616</td>\n",
       "      <td>bert-base-uncased-hatexplain</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.141935</td>\n",
       "      <td>0.334191</td>\n",
       "      <td>0.854430</td>\n",
       "      <td>0.854430</td>\n",
       "      <td>0.854430</td>\n",
       "      <td>0.328314</td>\n",
       "      <td>0.848276</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.842466</td>\n",
       "      <td>bert-base-uncased-hatexplain</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  dev_loss    dev_f1  dev_precision  dev_recall  \\\n",
       "0    1.0    0.590648  0.305677  0.839344       0.870748    0.810127   \n",
       "1    2.0    0.261547  0.286005  0.859843       0.855799    0.863924   \n",
       "2    3.0    0.180992  0.301318  0.855769       0.866883    0.844937   \n",
       "3    4.0    0.141935  0.334191  0.854430       0.854430    0.854430   \n",
       "\n",
       "   test_loss   test_f1  test_precision  test_recall  \\\n",
       "0   0.313553  0.812057        0.841912     0.784247   \n",
       "1   0.281830  0.825127        0.818182     0.832192   \n",
       "2   0.304679  0.847222        0.859155     0.835616   \n",
       "3   0.328314  0.848276        0.854167     0.842466   \n",
       "\n",
       "                          model train_len          train_set(s)  \\\n",
       "0  bert-base-uncased-hatexplain      3642  train_incelsis(3642)   \n",
       "1  bert-base-uncased-hatexplain      3642  train_incelsis(3642)   \n",
       "2  bert-base-uncased-hatexplain      3642  train_incelsis(3642)   \n",
       "3  bert-base-uncased-hatexplain      3642  train_incelsis(3642)   \n",
       "\n",
       "          dev_set(s)         test_set(s)  run_id  \n",
       "0  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "1  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "2  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "3  dev_incelsis(780)  test_incelsis(781)       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "# write set identifiers for the pandas metrics dataframe\n",
    "df_metrics_train_set_string=''\n",
    "for i,index in enumerate(df_train['data_type'].value_counts(normalize = False).index.to_list()):\n",
    "    set_len=df_train['data_type'].value_counts(normalize = False).values[i]\n",
    "    df_metrics_train_set_string+=index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "df_metrics_dev_set_string=''\n",
    "for i,index in enumerate(df_dev['data_type'].value_counts(normalize = False).index.to_list()):\n",
    "    set_len=df_dev['data_type'].value_counts(normalize = False).values[i]\n",
    "    df_metrics_dev_set_string+=index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "df_metrics_test_set_string=''\n",
    "for i,index in enumerate(df_test['data_type'].value_counts(normalize = False).index.to_list()):\n",
    "    set_len=df_test['data_type'].value_counts(normalize = False).values[i]\n",
    "    df_metrics_test_set_string+=index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "# train\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize = False))\n",
    "print('Dev set length:', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize = False))\n",
    "print('Test set length:', len(df_test), '\\n')\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'dev_loss', 'dev_f1', 'dev_precision', 'dev_recall', 'test_loss', 'test_f1', 'test_precision', 'test_recall'])\n",
    "for epoch in range(1, epochs + 1):\n",
    "    model.train()  # set the model in training mode\n",
    "\n",
    "    loss_train_total = 0  # initialize the total training loss\n",
    "\n",
    "    # Create a progress bar for the training dataloader\n",
    "    progress_bar = tqdm(dataloader_train,\n",
    "                        desc=model_name_simple+' - Epoch {:1d}'.format(epoch),\n",
    "                        leave=False,\n",
    "                        disable=False\n",
    "                        )\n",
    "\n",
    "    # Loop over the batches in the training dataloader\n",
    "    for batch in progress_bar:\n",
    "        model.zero_grad()  # reset the gradients to 0 for each batch\n",
    "        batch = tuple(b.to(device) for b in batch)  # move the batch to the device (e.g. GPU)\n",
    "        inputs = {\n",
    "                'input_ids': batch[0],  # input_ids are the token ids\n",
    "                'attention_mask': batch[1],  # attention_mask masks the padding tokens\n",
    "                'labels': batch[2]\n",
    "                }  # the true labels of the input\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs[0]  # the first element of outputs is the loss\n",
    "\n",
    "        if multi_gpu:\n",
    "            loss=loss.mean()\n",
    "\n",
    "        loss_train_total += loss.item()  # accumulate the training loss\n",
    "\n",
    "        loss.backward()  # backpropagate the loss through the model to compute gradients\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the gradients to prevent explosion\n",
    "\n",
    "        optimizer.step()  # update the model parameters using the computed gradients\n",
    "        scheduler.step()  # update the learning rate using the learning rate scheduler\n",
    "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item() / len(batch))})  # update the progress bar to show the current loss\n",
    "\n",
    "    loss_train_avg = loss_train_total / len(dataloader_train)\n",
    "\n",
    "    dev_loss, pred_val, true_values_val = evaluate(dataloader_val, prob_type, multi_gpu)  # to check overtraining (or overfitting)\n",
    "    dev_f1 = f1_score_func(pred_val, true_values_val, prob_type)\n",
    "    dev_prec = prec_func(pred_val, true_values_val, prob_type)\n",
    "    dev_recall = recall_func(pred_val, true_values_val, prob_type)\n",
    "\n",
    "    test_loss, pred_test, true_values_test = evaluate(dataloader_test, prob_type, multi_gpu)  # to check overtraining (or overfitting)\n",
    "    test_f1 = f1_score_func(pred_test, true_values_test, prob_type)\n",
    "    test_prec = prec_func(pred_test, true_values_test, prob_type)\n",
    "    test_recall = recall_func(pred_test, true_values_test, prob_type)\n",
    "\n",
    "    df_metrics = df_metrics.append({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': loss_train_avg,\n",
    "        'dev_loss': dev_loss,\n",
    "        'dev_f1': dev_f1,\n",
    "        'dev_precision': dev_prec,\n",
    "        'dev_recall': dev_recall,\n",
    "        'test_loss': test_loss,\n",
    "        'test_f1': test_f1,\n",
    "        'test_precision': test_prec,\n",
    "        'test_recall': test_recall\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    df_metrics['model']=model_name_simple\n",
    "    df_metrics['train_len']=str(len(df_train))\n",
    "    df_metrics['train_set(s)']=df_metrics_train_set_string[:-1]\n",
    "    df_metrics['dev_set(s)']=df_metrics_dev_set_string[:-1]\n",
    "    df_metrics['test_set(s)']=df_metrics_test_set_string[:-1]\n",
    "    df_metrics['run_id']=metrics_id\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print('Run ID:', metrics_id)\n",
    "    print('Train sets:')\n",
    "    print(df_train['data_type'].value_counts(normalize = False))\n",
    "    print('Train set length:', len(df_train), '\\n')\n",
    "    print('Dev sets:')\n",
    "    print(df_dev['data_type'].value_counts(normalize = False))\n",
    "    print('Dev set length:', len(df_dev), '\\n')\n",
    "    print('Test sets:')\n",
    "    print(df_test['data_type'].value_counts(normalize = False))\n",
    "    print('Test set length:', len(df_test), '\\n')\n",
    "    \n",
    "    display(df_metrics)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_metrics.to_csv(metrics_csv_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move the model to CPU\n",
    "# model = model.cpu()\n",
    "\n",
    "# # Move the tensors to CPU\n",
    "# input_ids_train = input_ids_train.cpu()\n",
    "# attention_masks_train = attention_masks_train.cpu()\n",
    "# labels_train = labels_train.cpu()\n",
    "\n",
    "# input_ids_val = input_ids_val.cpu()\n",
    "# attention_masks_val = attention_masks_val.cpu()\n",
    "# labels_val = labels_val.cpu()\n",
    "\n",
    "# input_ids_test = input_ids_test.cpu()\n",
    "# attention_masks_test = attention_masks_test.cpu()\n",
    "# labels_test = labels_test.cpu()\n",
    "\n",
    "# # Delete the tensors\n",
    "# del input_ids_train\n",
    "# del attention_masks_train\n",
    "# del labels_train\n",
    "\n",
    "# del input_ids_val\n",
    "# del attention_masks_val\n",
    "# del labels_val\n",
    "\n",
    "# del input_ids_test\n",
    "# del attention_masks_test\n",
    "# del labels_test\n",
    "\n",
    "# # Empty the GPU cache\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgajo-Fz_qUQZq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca09a575e78e41b281752c78b59ffb95d2982f5008f73e33df6571d672f258d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
