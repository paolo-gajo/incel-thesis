{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel, BertPreTrainedModel, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from pgfuncs import tokenize_and_vectorize, pad_trunc, collect_expected, tokenize_and_vectorize_1dlist, collect_expected_1dlist, df_classification_report\n",
    "\n",
    "from datetime import datetime\n",
    "# timestamp for file naming\n",
    "now = datetime.now()\n",
    "time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "date_str = now.strftime(\"%Y-%m-%d\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # used to make train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load incelsis_5203 dataset\n",
    "df_incelsis_5203 = pd.read_csv('/home/pgajo/working/data/datasets/English/Incels.is/IFD-EN-5203_splits.csv')\n",
    "\n",
    "df_train_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'train_incelsis']\n",
    "df_dev_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'dev_incelsis']\n",
    "df_test_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'test_incelsis']\n",
    "\n",
    "# Print the size of each split\n",
    "print('Incels.is train set size:', len(df_train_incelsis_5203))\n",
    "print('Incels.is dev set size:', len(df_dev_incelsis_5203))\n",
    "print('Incels.is test set size:', len(df_test_incelsis_5203))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load fdb_250 dataset\n",
    "df_fdb_250 = pd.read_csv('/home/pgajo/working/data/datasets/Italian/Il_forum_dei_brutti/IFD-IT-250.csv')\n",
    "df_fdb_250 = df_fdb_250[['hs','text']]\n",
    "df_fdb_250\n",
    "df_fdb_250['data_type']='test_fdb_250'\n",
    "\n",
    "print('Forum dei brutti test set size:', len(df_fdb_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the davidson set\n",
    "file_path_csv_davidson = '/home/pgajo/working/data/datasets/English/hate-speech-and-offensive-language (davidson)/davidson_labeled_data.csv'\n",
    "df_davidson = pd.read_csv(file_path_csv_davidson, index_col=None)\n",
    "df_davidson = df_davidson[['hs','text']]\n",
    "df_davidson['data_type']='davidson'\n",
    "df_davidson = df_davidson.sample(frac=1).reset_index(drop=True) # shuffle the set\n",
    "mask = df_davidson['hs'] >= 1\n",
    "\n",
    "# Set those values to 1\n",
    "df_davidson.loc[mask, 'hs'] = 1\n",
    "\n",
    "# Split the data into training and test sets (70% for training, 30% for test)\n",
    "df_train_davidson, df_test_davidson = train_test_split(df_davidson, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_davidson, df_test_davidson = train_test_split(df_test_davidson, test_size=0.5, random_state=42)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize=True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_davidson[df_train_davidson['hs'] == 1].sample(n=num_hs_1, replace=True)\n",
    "df_hs_0 = df_train_davidson[df_train_davidson['hs'] == 0].sample(n=num_hs_0, replace=True)\n",
    "df_train_davidson_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=True))\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=False))\n",
    "\n",
    "# Print the sample\n",
    "print('df_train_davidson_sample value_counts:')\n",
    "print(df_train_davidson_sample['hs'].value_counts(normalize=False))\n",
    "print()\n",
    "\n",
    "# Print the size of each split\n",
    "df_train_davidson['data_type']='train_davidson'\n",
    "df_dev_davidson['data_type']='dev_davidson'\n",
    "df_test_davidson['data_type']='test_davidson'\n",
    "print('Davidson full train set size:', len(df_train_davidson))\n",
    "print('Davidson full dev set size:', len(df_dev_davidson))\n",
    "print('Davidson full test set size:', len(df_test_davidson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hateval_2019_english set\n",
    "file_path_csv_hateval_2019_english_train = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_train_miso.csv'\n",
    "file_path_csv_hateval_2019_english_dev = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_dev_miso.csv'\n",
    "file_path_csv_hateval_2019_english_test = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_test_miso.csv'\n",
    "\n",
    "df_train_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_train, index_col = None)\n",
    "df_dev_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_dev, index_col = None)\n",
    "df_test_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_test, index_col = None)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_english_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval english sample value_counts:')\n",
    "print(df_train_hateval_2019_english_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_english['data_type']='train_hateval_2019_english'\n",
    "df_dev_hateval_2019_english['data_type']='dev_hateval_2019_english'\n",
    "df_test_hateval_2019_english['data_type']='test_hateval_2019_english'\n",
    "print('HatEval english full train set size:', len(df_train_hateval_2019_english))\n",
    "print('HatEval english full dev set size:', len(df_dev_hateval_2019_english))\n",
    "print('HatEval english full test set size:', len(df_test_hateval_2019_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the hateval_2019_spanish set\n",
    "file_path_csv_hateval_2019_spanish_train = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_train.csv'\n",
    "file_path_csv_hateval_2019_spanish_dev = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_dev.csv'\n",
    "file_path_csv_hateval_2019_spanish_test = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_test.csv'\n",
    "\n",
    "df_train_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_train, index_col = None)\n",
    "df_train_hateval_2019_spanish = df_train_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_dev_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_dev, index_col = None)\n",
    "df_dev_hateval_2019_spanish = df_dev_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_test_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_test, index_col = None)\n",
    "df_test_hateval_2019_spanish = df_test_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_spanish_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval spanish sample value_counts:')\n",
    "print(df_train_hateval_2019_spanish_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_spanish['data_type']='train_hateval_2019_spanish'\n",
    "df_dev_hateval_2019_spanish['data_type']='dev_hateval_2019_spanish'\n",
    "df_test_hateval_2019_spanish['data_type']='test_hateval_2019_spanish'\n",
    "print('HatEval spanish full train set size:', len(df_train_hateval_2019_spanish))\n",
    "print('HatEval spanish full dev set size:', len(df_dev_hateval_2019_spanish))\n",
    "print('HatEval spanish full test set size:', len(df_test_hateval_2019_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HateXplain dataset\n",
    "import json\n",
    "filename_json = '/home/pgajo/working/data/datasets/English/HateXplain/Data/dataset.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(filename_json, 'r') as f:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "def post_majority_vote_choice(label_list):\n",
    "    '''\n",
    "    Returns the majority vote for a post in the HateXplain json dataset.\n",
    "    '''\n",
    "    label_dict={}\n",
    "    for i,post_label in enumerate(label_list):\n",
    "        # print(i,post_label)\n",
    "        if post_label not in label_dict:\n",
    "            label_dict[post_label]=1\n",
    "        else:\n",
    "            label_dict[post_label]+=1\n",
    "    max_key = max(label_dict, key=label_dict.get)\n",
    "    if label_dict[max_key]>1:\n",
    "        return max_key # return the label key with the highest value if > 1\n",
    "\n",
    "df_hatexplain_list = []\n",
    "for key_post in dataset_json.keys():\n",
    "    post = []\n",
    "    labels_post = [key_annotators['label'] for key_annotators in dataset_json[key_post]['annotators']] # get the list of labels\n",
    "    label_majority=post_majority_vote_choice(labels_post) # return the majority label\n",
    "    if label_majority!=None: # the post_majority_vote_choice returns None if there is no majority label, i.e., they all have the same occurrences\n",
    "        post.append(label_majority) # append the label of the post\n",
    "        post.append(' '.join(dataset_json[key_post]['post_tokens'])) # append the text tokens of the post\n",
    "        df_hatexplain_list.append(post) # append the label-text pair\n",
    "df_hatexplain=pd.DataFrame(df_hatexplain_list, columns=['hs','text'])\n",
    "df_hatexplain_binary = df_hatexplain.loc[df_hatexplain['hs'] != 'offensive']\n",
    "df_hatexplain_binary['hs'] = df_hatexplain_binary['hs'].replace({'normal': 0, 'hatespeech': 1})\n",
    "# df_hatexplain_binary\n",
    "# Split the data into training and test sets (80% for training, 20% for test)\n",
    "hatexplain_binary_devtest_size=0.2\n",
    "df_train_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_hatexplain_binary, test_size=hatexplain_binary_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_test_hatexplain_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_hatexplain_binary['data_type']='hatexplain_binary_train'\n",
    "df_dev_hatexplain_binary['data_type']='hatexplain_binary_dev'\n",
    "df_test_hatexplain_binary['data_type']='hatexplain_binary_test'\n",
    "print('HateXplain binary dev+test split ratio:',hatexplain_binary_devtest_size)\n",
    "print('HateXplain binary full train set size:', len(df_train_hatexplain_binary))\n",
    "print('HateXplain binary full dev set size:', len(df_dev_hatexplain_binary))\n",
    "print('HateXplain binary full test set size:', len(df_test_hatexplain_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stormfront dataset from \"Hate speech dataset from a white supremacist forum\"\n",
    "\n",
    "df_stormfront_raw=pd.read_csv('/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/annotations_metadata.csv')\n",
    "df_stormfront_raw['label'] = df_stormfront_raw['label'].replace({'noHate': 0, 'hate': 1})\n",
    "df_stormfront_raw = df_stormfront_raw.rename(columns={'label': 'hs'})\n",
    "\n",
    "post_dir='/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/all_files'\n",
    "dict_ids_labels={}\n",
    "dict_post_pairs_ws=[]\n",
    "\n",
    "for row in df_stormfront_raw.values.tolist():\n",
    "    dict_ids_labels[row[0]]=row[4]\n",
    "len(dict_ids_labels)\n",
    "for filename in os.listdir(post_dir):\n",
    "    with open(os.path.join(post_dir, filename), 'r') as file:\n",
    "        # Read the contents of the file into a string variable\n",
    "        file_contents = file.read()\n",
    "        filename=filename[:-4]\n",
    "    dict_post_pairs_ws.append([dict_ids_labels[filename],file_contents,filename])\n",
    "df_stormfront=pd.DataFrame(dict_post_pairs_ws, columns=['hs','text','filename'])\n",
    "df_stormfront = df_stormfront[(df_stormfront['hs'] == 0) | (df_stormfront['hs'] == 1)]\n",
    "df_stormfront['hs']=df_stormfront['hs'].astype(int)\n",
    "\n",
    "# Split the data into training and test sets (80% for training, 30% for test)\n",
    "df_stormfront_devtest_size=0.3\n",
    "df_train_stormfront, df_test_stormfront = train_test_split(df_stormfront, test_size=df_stormfront_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_stormfront, df_test_stormfront = train_test_split(df_test_stormfront, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_stormfront['data_type']='df_stormfront_train'\n",
    "df_dev_stormfront['data_type']='df_stormfront_dev'\n",
    "df_test_stormfront['data_type']='df_stormfront_test'\n",
    "print('Stormfront dataset dev+test split size:',df_stormfront_devtest_size)\n",
    "print('Stormfront dataset train set size:', len(df_train_stormfront))\n",
    "print('Stormfront dataset dev set size:', len(df_dev_stormfront))\n",
    "print('Stormfront dataset test set size:', len(df_test_stormfront))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evalita18twitter set\n",
    "file_path_csv_evalita18twitter_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/TW-folder-20230313T173228Z-001/TW-folder/TW-train/haspeede_TW-train.tsv'\n",
    "\n",
    "df_train_evalita18twitter = pd.read_csv(file_path_csv_evalita18twitter_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18twitter.columns=['id','text','hs']\n",
    "# display(df_train_evalita18twitter)\n",
    "df_train_evalita18twitter['data_type'] = 'train_evalita18twitter'\n",
    "print('evalita18twitter full train set size:', len(df_train_evalita18twitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evalita18facebook set\n",
    "file_path_csv_evalita18facebook_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/FB-folder-20230313T173818Z-001/FB-folder/FB-train/haspeede_FB-train.tsv'\n",
    "\n",
    "df_train_evalita18facebook = pd.read_csv(file_path_csv_evalita18facebook_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18facebook['data_type'] = 'train_evalita18facebook'\n",
    "# display(df_train_evalita18facebook)\n",
    "print('evalita18facebook full train set size:', len(df_train_evalita18facebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the evalita20 set\n",
    "file_path_csv_evalita20_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2020/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "df_train_evalita20 = pd.read_csv(file_path_csv_evalita20_train, sep='\\t', index_col = None)\n",
    "# display(df_train_evalita20)\n",
    "\n",
    "print('evalita20 full train set size:', len(df_train_evalita20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the offenseval_2020 dataset\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# configs = ['ar', 'da', 'en', 'gr', 'tr']\n",
    "# datasets = {}\n",
    "\n",
    "# for config in configs:\n",
    "#     datasets[config] = load_dataset(\"strombergnlp/offenseval_2020\", config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_id=0\n",
    "device_index = -1 # set to -1 for multigpu # Set the index of the CUDA device you want to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "\n",
    "# set problem type\n",
    "prob_type = 'binary'\n",
    "\n",
    "# set task name\n",
    "task_name = 'incelsis'\n",
    "\n",
    "# define dataset combinations\n",
    "metrics_list_names=[\n",
    "    # monolingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_incelsis_5203'], # 0\n",
    "    ['train_incelsis_5203+train_davidson_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 1\n",
    "    ['train_incelsis_5203+train_hateval_2019_english_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 2\n",
    "    ['train_incelsis_5203+train_davidson_sample+train_hateval_2019_english_sample', 'dev_incelsis_5203', 'test_incelsis_5203'], # 3\n",
    "    ['train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 4\n",
    "    ['train_hateval_2019_english+train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'], # 5\n",
    "    ['train_incelsis_5203', 'dev_hateval_2019_english', 'test_hateval_2019_english'], # 6\n",
    "    ['train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'], # 7\n",
    "    ['train_incelsis_5203', 'dev_davidson', 'test_davidson'], # 8\n",
    "    ['train_incelsis_5203+train_davidson+train_hateval_2019_english', 'dev_davidson', 'test_davidson'], # 9\n",
    "    ['train_incelsis_5203+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 10\n",
    "    ['train_hatexplain_binary', 'hatexplain_binary_dev', 'hatexplain_binary_test'], # 11\n",
    "    ['train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'], # 12\n",
    "    ['train_incelsis_5203+train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'], # 13\n",
    "    ['train_incelsis_5203+train_hatexplain_binary+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 14\n",
    "    ['train_incelsis_5203+train_stormfront', 'dev_incelsis_5203', 'test_incelsis_5203'], # 15\n",
    "    ['train_incelsis_5203+train_stormfront+train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'], # 16\n",
    "\n",
    "    # multilingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 17\n",
    "    ['train_incelsis_5203+train_hateval_2019_english', 'dev_incelsis_5203', 'test_fdb_250'], # 18\n",
    "    ['train_incelsis_5203+train_hateval_2019_spanish', 'dev_incelsis_5203', 'test_fdb_250'], # 19\n",
    "    ['train_incelsis_5203+train_hateval_2019_english+train_hateval_2019_spanish', 'dev_incelsis_5203', 'test_fdb_250'], # 20\n",
    "    ['train_incelsis_5203+train_evalita18facebook', 'dev_incelsis_5203', 'test_fdb_250'], # 21\n",
    "    ['train_incelsis_5203+train_evalita18twitter', 'dev_incelsis_5203', 'test_fdb_250'], # 22\n",
    "    ['train_incelsis_5203+train_evalita18facebook+train_evalita18twitter', 'dev_incelsis_5203', 'test_fdb_250'], # 23\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 24\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'], # 25\n",
    "\n",
    "]\n",
    "\n",
    "# set train datasets\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "if 'incelsis' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_incelsis_5203])\n",
    "\n",
    "if 'davidson' in metrics_list_names[metrics_id][0]:\n",
    "    if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "        df_train = pd.concat([df_train,df_train_davidson_sample])\n",
    "    else:\n",
    "        df_train = pd.concat([df_train,df_train_davidson])\n",
    "\n",
    "if 'hateval' in metrics_list_names[metrics_id][0]:\n",
    "    if 'english' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train,df_train_hateval_2019_spanish])\n",
    "\n",
    "if 'train_hatexplain_binary' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_hatexplain_binary])\n",
    "\n",
    "if 'train_stormfront' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_stormfront])\n",
    "\n",
    "if 'train_evalita18facebook' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_evalita18facebook])\n",
    "\n",
    "if 'train_evalita18twitter' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train,df_train_evalita18twitter])\n",
    "\n",
    "df_dev = pd.DataFrame()\n",
    "# set dev datasets\n",
    "if 'dev_incelsis_5203' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_incelsis_5203])\n",
    "\n",
    "if 'dev_davidson' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_davidson])\n",
    "\n",
    "if 'dev_hateval_2019' in metrics_list_names[metrics_id][1]:\n",
    "    if 'english' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev,df_dev_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev,df_dev_hateval_2019_spanish])\n",
    "\n",
    "if 'dev_hatexplain_binary' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_hatexplain_binary])\n",
    "\n",
    "if 'dev_stormfront' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev,df_dev_stormfront])\n",
    "\n",
    "# set test datasets\n",
    "if 'test_incelsis_5203' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_incelsis_5203\n",
    "\n",
    "if 'test_davidson' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_davidson\n",
    "\n",
    "if 'test_hateval_2019' in metrics_list_names[metrics_id][2]:\n",
    "    if 'english' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_english\n",
    "    if 'spanish' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_spanish\n",
    "\n",
    "if 'test_hatexplain_binary' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_hatexplain_binary\n",
    "\n",
    "if 'test_stormfront' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_stormfront\n",
    "\n",
    "if 'test_fdb_250' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_fdb_250\n",
    "\n",
    "df_train = df_train.sample(frac = 1)\n",
    "df_dev= df_dev.sample(frac = 1)\n",
    "\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize = False))\n",
    "print('Train set length:', len(df_dev), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertForSequenceClassification, BertConfig, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "\n",
    "# monolingual models\n",
    "model_name = 'bert-base-uncased'\n",
    "# model_name = '/home/pgajo/working/pt_models/HateBERT'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-100k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert-1M'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-bert'\n",
    "# model_name = 'Hate-speech-CNERG/bert-base-uncased-hatexplain'\n",
    "# model_name = 'roberta-base'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-roberta-base-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-roberta-base-100k'\n",
    "\n",
    "# multilingual models\n",
    "# model_name = 'bert-base-multilingual-cased'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-10k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-100k'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert-1M'\n",
    "# model_name = '/home/pgajo/working/pt_models/incel-mbert'\n",
    "# model_name = 'xlm-roberta-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model_name_simple=model_name.split('/')[-1]\n",
    "# print(model.eval())\n",
    "# print(model.config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the training data using the tokenizer\n",
    "encoded_data_train = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_train.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encode the validation data using the tokenizer\n",
    "encoded_data_val = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_dev.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")\n",
    "\n",
    "# Encode the validation data using the tokenizer\n",
    "encoded_data_test = tokenizer.batch_encode_plus(\n",
    "    [el for el in tqdm(df_test.text.values)],  # text to encode, wrapped in a tqdm progress bar to show progress\n",
    "    add_special_tokens=True,  # add special tokens to mark the beginning and end of each sentence\n",
    "    return_attention_mask=True,  # generate attention masks to distinguish padding from actual tokens\n",
    "    pad_to_max_length=True,  # pad each sentence to the maximum length\n",
    "    max_length=256,  # set the maximum length of each sentence to 256\n",
    "    return_tensors='pt'  # return PyTorch tensors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract IDs, attention masks and labels from training dataset\n",
    "input_ids_train = encoded_data_train['input_ids']\n",
    "attention_masks_train = encoded_data_train['attention_mask']\n",
    "labels_train = torch.tensor(df_train.hs.values)\n",
    "# Extract IDs, attention masks and labels from validation dataset\n",
    "input_ids_val = encoded_data_val['input_ids']\n",
    "attention_masks_val = encoded_data_val['attention_mask']\n",
    "labels_val = torch.tensor(df_dev.hs.values)\n",
    "# Extract IDs, attention masks and labels from test dataset\n",
    "input_ids_test = encoded_data_test['input_ids']\n",
    "attention_masks_test = encoded_data_test['attention_mask']\n",
    "labels_test = torch.tensor(df_test.hs.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create train and validation dataset from extracted features\n",
    "# from torch.utils.data import TensorDataset\n",
    "# dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "# dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "# dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "# print(\"Train set length: {}\\nDev set length: {}\\nTest set length: {}\".format(len(dataset_train), len(dataset_val), len(dataset_test)))\n",
    "\n",
    "# # Define the size of each batch\n",
    "# batch_size = 16  # number of examples to include in each batch\n",
    "\n",
    "# # Load training dataset\n",
    "# dataloader_train= DataLoader(\n",
    "#     dataset_train,  # training dataset to load\n",
    "#     sampler=RandomSampler(dataset_train),  # randomly sample examples from the training dataset\n",
    "#     batch_size=batch_size  # set the batch size to the defined value\n",
    "# )\n",
    "\n",
    "# # Load valuation dataset\n",
    "# dataloader_val= DataLoader(\n",
    "#     dataset_val,  # valuation dataset to load\n",
    "#     sampler=RandomSampler(dataset_val),  # randomly sample examples from the valuation dataset\n",
    "#     batch_size=batch_size  # set the batch size to the defined value\n",
    "# )\n",
    "\n",
    "# # Load test dataset\n",
    "# dataloader_test= DataLoader(\n",
    "#     dataset_test,  # testuation dataset to load\n",
    "#     sampler=RandomSampler(dataset_test),  # randomly sample examples from the valuation dataset\n",
    "#     batch_size=batch_size  # set the batch size to the defined value\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4  # number of epochs\n",
    "# Define the size of each batch\n",
    "batch_size = 16  # number of examples to include in each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {'input_ids': self.input_ids[idx], 'attention_mask': self.attention_masks[idx], 'labels': self.labels[idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "# Create the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    precision = precision_score(labels, preds, average='binary')\n",
    "    recall = recall_score(labels, preds, average='binary')\n",
    "\n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom dataset instances\n",
    "train_dataset = CustomDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "val_dataset = CustomDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "test_dataset = CustomDataset(input_ids_test, attention_masks_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename bits\n",
    "multilingual=0\n",
    "if multilingual:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_multilingual'\n",
    "else:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_monolingual'\n",
    "\n",
    "metrics_save_path_model = os.path.join(metrics_save_path, model_name_simple)\n",
    "\n",
    "if not os.path.exists(metrics_save_path_model):\n",
    "    os.mkdir(metrics_save_path_model)\n",
    "\n",
    "metrics_filename = str(metrics_id)+'_'+model_name_simple+'_'+time_str+'_metrics.csv'\n",
    "metrics_csv_filepath = os.path.join(metrics_save_path_model, metrics_filename)\n",
    "print(metrics_csv_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',           # Output directory for model and predictions\n",
    "    num_train_epochs=epochs,          # Number of epochs\n",
    "    per_device_train_batch_size=batch_size,    # Batch size per device during training\n",
    "    per_device_eval_batch_size=batch_size,     # Batch size per device during evaluation\n",
    "    warmup_steps=0,                  # Linear warmup over warmup_steps\n",
    "    weight_decay=0.01,               # Weight decay\n",
    "    logging_dir='./logs',            # Directory for storing logs\n",
    "    logging_steps=100,               # Log every X updates steps\n",
    "    evaluation_strategy='epoch',     # Evaluate every epoch\n",
    "    save_strategy='epoch',              # Do not save checkpoint after each epoch\n",
    "    load_best_model_at_end=True,     # Load the best model when finished training (best on dev set)\n",
    "    metric_for_best_model='f1',      # Use f1 score to determine the best model\n",
    "    greater_is_better=True           # The higher the f1 score, the better\n",
    ")\n",
    "\n",
    "# Define the Hugging Face Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # The pre-trained model\n",
    "    args=training_args,                  # Training arguments\n",
    "    train_dataset=train_dataset,         # Training dataset\n",
    "    eval_dataset=val_dataset,            # Evaluation dataset\n",
    "    compute_metrics=compute_metrics      # Custom metrics function (to be defined)\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(test_results)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "# df_metrics.to_csv(metrics_csv_filepath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgajo-Fz_qUQZq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca09a575e78e41b281752c78b59ffb95d2982f5008f73e33df6571d672f258d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
