{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 02:15:21.606479: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 02:15:21.716391: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-28 02:15:21.716414: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-28 02:15:22.280184: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 02:15:22.280247: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 02:15:22.280253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, TFBertModel, BertPreTrainedModel, BertModel, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, precision_score, recall_score, log_loss\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "from pgfuncs import tokenize_and_vectorize, pad_trunc, collect_expected, tokenize_and_vectorize_1dlist, collect_expected_1dlist, df_classification_report\n",
    "\n",
    "from datetime import datetime\n",
    "# timestamp for file naming\n",
    "now = datetime.now()\n",
    "time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "date_str = now.strftime(\"%Y-%m-%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split # used to make train/dev/test partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incels.is train set size: 3642\n",
      "Incels.is dev set size: 780\n",
      "Incels.is test set size: 781\n"
     ]
    }
   ],
   "source": [
    "# load incelsis_5203 dataset\n",
    "df_incelsis_5203 = pd.read_csv('/home/pgajo/working/data/datasets/English/Incels.is/IFD-EN-5203_splits.csv')\n",
    "\n",
    "df_train_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'train_incelsis']\n",
    "df_dev_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'dev_incelsis']\n",
    "df_test_incelsis_5203 = df_incelsis_5203[df_incelsis_5203['data_type'] == 'test_incelsis']\n",
    "\n",
    "# Print the size of each split\n",
    "print('Incels.is train set size:', len(df_train_incelsis_5203))\n",
    "print('Incels.is dev set size:', len(df_dev_incelsis_5203))\n",
    "print('Incels.is test set size:', len(df_test_incelsis_5203))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forum dei brutti test set size: 250\n"
     ]
    }
   ],
   "source": [
    "# load fdb_250 dataset\n",
    "df_fdb_250 = pd.read_csv('/home/pgajo/working/data/datasets/Italian/Il_forum_dei_brutti/IFD-IT-250.csv')\n",
    "df_fdb_250 = df_fdb_250[['hs','text']]\n",
    "df_fdb_250\n",
    "df_fdb_250['data_type']='test_fdb_250'\n",
    "\n",
    "print('Forum dei brutti test set size:', len(df_fdb_250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_davidson_sample value_counts:\n",
      "0    2160\n",
      "1    1482\n",
      "Name: hs, dtype: int64\n",
      "\n",
      "Davidson full train set size: 17348\n",
      "Davidson full dev set size: 3717\n",
      "Davidson full test set size: 3718\n"
     ]
    }
   ],
   "source": [
    "# load the davidson set\n",
    "file_path_csv_davidson = '/home/pgajo/working/data/datasets/English/hate-speech-and-offensive-language (davidson)/davidson_labeled_data.csv'\n",
    "df_davidson = pd.read_csv(file_path_csv_davidson, index_col=None)\n",
    "df_davidson = df_davidson[['hs','text']]\n",
    "df_davidson['data_type']='davidson'\n",
    "df_davidson = df_davidson.sample(frac=1).reset_index(drop=True) # shuffle the set\n",
    "mask = df_davidson['hs'] >= 1\n",
    "\n",
    "# Set those values to 1\n",
    "df_davidson.loc[mask, 'hs'] = 1\n",
    "\n",
    "# Split the data into training and test sets (70% for training, 30% for test)\n",
    "df_train_davidson, df_test_davidson = train_test_split(df_davidson, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_davidson, df_test_davidson = train_test_split(df_test_davidson, test_size=0.5, random_state=42)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize=True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_davidson[df_train_davidson['hs'] == 1].sample(n=num_hs_1, replace=True)\n",
    "df_hs_0 = df_train_davidson[df_train_davidson['hs'] == 0].sample(n=num_hs_0, replace=True)\n",
    "df_train_davidson_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=True))\n",
    "# print(df_sample_davidson['hs'].value_counts(normalize=False))\n",
    "\n",
    "# Print the sample\n",
    "print('df_train_davidson_sample value_counts:')\n",
    "print(df_train_davidson_sample['hs'].value_counts(normalize=False))\n",
    "print()\n",
    "\n",
    "# Print the size of each split\n",
    "df_train_davidson['data_type']='train_davidson'\n",
    "df_dev_davidson['data_type']='dev_davidson'\n",
    "df_test_davidson['data_type']='test_davidson'\n",
    "print('Davidson full train set size:', len(df_train_davidson))\n",
    "print('Davidson full dev set size:', len(df_dev_davidson))\n",
    "print('Davidson full test set size:', len(df_test_davidson))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HatEval english sample value_counts:\n",
      "0    2160\n",
      "1    1482\n",
      "Name: hs, dtype: int64\n",
      "\n",
      "HatEval english full train set size: 4500\n",
      "HatEval english full dev set size: 500\n",
      "HatEval english full test set size: 1500\n"
     ]
    }
   ],
   "source": [
    "# load the hateval_2019_english set\n",
    "file_path_csv_hateval_2019_english_train = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_train_miso.csv'\n",
    "file_path_csv_hateval_2019_english_dev = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_dev_miso.csv'\n",
    "file_path_csv_hateval_2019_english_test = '/home/pgajo/working/data/datasets/English/hateval2019_en/hateval2019_en_test_miso.csv'\n",
    "\n",
    "df_train_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_train, index_col = None)\n",
    "df_dev_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_dev, index_col = None)\n",
    "df_test_hateval_2019_english = pd.read_csv(file_path_csv_hateval_2019_english_test, index_col = None)\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_english[df_train_hateval_2019_english['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_english_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval english sample value_counts:')\n",
    "print(df_train_hateval_2019_english_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_english['data_type']='train_hateval_2019_english'\n",
    "df_dev_hateval_2019_english['data_type']='dev_hateval_2019_english'\n",
    "df_test_hateval_2019_english['data_type']='test_hateval_2019_english'\n",
    "print('HatEval english full train set size:', len(df_train_hateval_2019_english))\n",
    "print('HatEval english full dev set size:', len(df_dev_hateval_2019_english))\n",
    "print('HatEval english full test set size:', len(df_test_hateval_2019_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HatEval spanish sample value_counts:\n",
      "0    2160\n",
      "1    1482\n",
      "Name: hs, dtype: int64\n",
      "\n",
      "HatEval spanish full train set size: 4500\n",
      "HatEval spanish full dev set size: 500\n",
      "HatEval spanish full test set size: 1600\n"
     ]
    }
   ],
   "source": [
    "# load the hateval_2019_spanish set\n",
    "file_path_csv_hateval_2019_spanish_train = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_train.csv'\n",
    "file_path_csv_hateval_2019_spanish_dev = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_dev.csv'\n",
    "file_path_csv_hateval_2019_spanish_test = '/home/pgajo/working/data/datasets/Spanish/hateval2019_es/hateval2019_es_test.csv'\n",
    "\n",
    "df_train_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_train, index_col = None)\n",
    "df_train_hateval_2019_spanish = df_train_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_dev_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_dev, index_col = None)\n",
    "df_dev_hateval_2019_spanish = df_dev_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "df_test_hateval_2019_spanish = pd.read_csv(file_path_csv_hateval_2019_spanish_test, index_col = None)\n",
    "df_test_hateval_2019_spanish = df_test_hateval_2019_spanish.rename(columns={'HS': 'hs'})\n",
    "\n",
    "# sample and get the same proportions of binary classes as the incels dataset\n",
    "\n",
    "# Set the desired proportions of 1's and 0's in the sample\n",
    "prop_1 = df_train_incelsis_5203['hs'].value_counts(normalize = True)[1]\n",
    "df_len=len(df_train_incelsis_5203)\n",
    "# Calculate the number of rows with 1s and 0s in the sample\n",
    "num_hs_1 = int(df_len * prop_1)\n",
    "num_hs_0 = df_len - num_hs_1\n",
    "\n",
    "# Select rows with 1s and 0s separately, and concatenate the results\n",
    "df_hs_1 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 1].sample(n = num_hs_1, replace = True)\n",
    "df_hs_0 = df_train_hateval_2019_spanish[df_train_hateval_2019_spanish['hs'] == 0].sample(n = num_hs_0, replace = True)\n",
    "df_train_hateval_2019_spanish_sample = pd.concat([df_hs_1, df_hs_0])\n",
    "\n",
    "# Print the sample\n",
    "print('HatEval spanish sample value_counts:')\n",
    "print(df_train_hateval_2019_spanish_sample['hs'].value_counts(normalize = False))\n",
    "print()\n",
    "df_train_hateval_2019_spanish['data_type']='train_hateval_2019_spanish'\n",
    "df_dev_hateval_2019_spanish['data_type']='dev_hateval_2019_spanish'\n",
    "df_test_hateval_2019_spanish['data_type']='test_hateval_2019_spanish'\n",
    "print('HatEval spanish full train set size:', len(df_train_hateval_2019_spanish))\n",
    "print('HatEval spanish full dev set size:', len(df_dev_hateval_2019_spanish))\n",
    "print('HatEval spanish full test set size:', len(df_test_hateval_2019_spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HateXplain binary dev+test split ratio: 0.2\n",
      "HateXplain binary full train set size: 10999\n",
      "HateXplain binary full dev set size: 1375\n",
      "HateXplain binary full test set size: 1375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2581803/361674652.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_hatexplain_binary['hs'] = df_hatexplain_binary['hs'].replace({'normal': 0, 'hatespeech': 1})\n"
     ]
    }
   ],
   "source": [
    "# load the HateXplain dataset\n",
    "import json\n",
    "filename_json = '/home/pgajo/working/data/datasets/English/HateXplain/Data/dataset.json'\n",
    "\n",
    "# Open the JSON file\n",
    "with open(filename_json, 'r') as f:\n",
    "    # Load the JSON data into a Python dictionary\n",
    "    dataset_json = json.load(f)\n",
    "\n",
    "def post_majority_vote_choice(label_list):\n",
    "    '''\n",
    "    Returns the majority vote for a post in the HateXplain json dataset.\n",
    "    '''\n",
    "    label_dict={}\n",
    "    for i,post_label in enumerate(label_list):\n",
    "        # print(i,post_label)\n",
    "        if post_label not in label_dict:\n",
    "            label_dict[post_label]=1\n",
    "        else:\n",
    "            label_dict[post_label]+=1\n",
    "    max_key = max(label_dict, key=label_dict.get)\n",
    "    if label_dict[max_key]>1:\n",
    "        return max_key # return the label key with the highest value if > 1\n",
    "\n",
    "df_hatexplain_list = []\n",
    "for key_post in dataset_json.keys():\n",
    "    post = []\n",
    "    labels_post = [key_annotators['label'] for key_annotators in dataset_json[key_post]['annotators']] # get the list of labels\n",
    "    label_majority=post_majority_vote_choice(labels_post) # return the majority label\n",
    "    if label_majority!=None: # the post_majority_vote_choice returns None if there is no majority label, i.e., they all have the same occurrences\n",
    "        post.append(label_majority) # append the label of the post\n",
    "        post.append(' '.join(dataset_json[key_post]['post_tokens'])) # append the text tokens of the post\n",
    "        df_hatexplain_list.append(post) # append the label-text pair\n",
    "df_hatexplain=pd.DataFrame(df_hatexplain_list, columns=['hs','text'])\n",
    "df_hatexplain_binary = df_hatexplain.loc[df_hatexplain['hs'] != 'offensive']\n",
    "df_hatexplain_binary['hs'] = df_hatexplain_binary['hs'].replace({'normal': 0, 'hatespeech': 1})\n",
    "# df_hatexplain_binary\n",
    "# Split the data into training and test sets (80% for training, 20% for test)\n",
    "hatexplain_binary_devtest_size=0.2\n",
    "df_train_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_hatexplain_binary, test_size=hatexplain_binary_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_hatexplain_binary, df_test_hatexplain_binary = train_test_split(df_test_hatexplain_binary, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_hatexplain_binary['data_type']='hatexplain_binary_train'\n",
    "df_dev_hatexplain_binary['data_type']='hatexplain_binary_dev'\n",
    "df_test_hatexplain_binary['data_type']='hatexplain_binary_test'\n",
    "print('HateXplain binary dev+test split ratio:',hatexplain_binary_devtest_size)\n",
    "print('HateXplain binary full train set size:', len(df_train_hatexplain_binary))\n",
    "print('HateXplain binary full dev set size:', len(df_dev_hatexplain_binary))\n",
    "print('HateXplain binary full test set size:', len(df_test_hatexplain_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stormfront dataset dev+test split size: 0.3\n",
      "Stormfront dataset train set size: 7492\n",
      "Stormfront dataset dev set size: 1605\n",
      "Stormfront dataset test set size: 1606\n"
     ]
    }
   ],
   "source": [
    "# load the stormfront dataset from \"Hate speech dataset from a white supremacist forum\"\n",
    "\n",
    "df_stormfront_raw=pd.read_csv('/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/annotations_metadata.csv')\n",
    "df_stormfront_raw['label'] = df_stormfront_raw['label'].replace({'noHate': 0, 'hate': 1})\n",
    "df_stormfront_raw = df_stormfront_raw.rename(columns={'label': 'hs'})\n",
    "\n",
    "post_dir='/home/pgajo/working/data/datasets/English/hate-speech-dataset-stormfront/all_files'\n",
    "dict_ids_labels={}\n",
    "dict_post_pairs_ws=[]\n",
    "\n",
    "for row in df_stormfront_raw.values.tolist():\n",
    "    dict_ids_labels[row[0]]=row[4]\n",
    "len(dict_ids_labels)\n",
    "for filename in os.listdir(post_dir):\n",
    "    with open(os.path.join(post_dir, filename), 'r') as file:\n",
    "        # Read the contents of the file into a string variable\n",
    "        file_contents = file.read()\n",
    "        filename=filename[:-4]\n",
    "    dict_post_pairs_ws.append([dict_ids_labels[filename],file_contents,filename])\n",
    "df_stormfront=pd.DataFrame(dict_post_pairs_ws, columns=['hs','text','filename'])\n",
    "df_stormfront = df_stormfront[(df_stormfront['hs'] == 0) | (df_stormfront['hs'] == 1)]\n",
    "df_stormfront['hs']=df_stormfront['hs'].astype(int)\n",
    "\n",
    "# Split the data into training and test sets (80% for training, 30% for test)\n",
    "df_stormfront_devtest_size=0.3\n",
    "df_train_stormfront, df_test_stormfront = train_test_split(df_stormfront, test_size=df_stormfront_devtest_size, random_state=42)\n",
    "\n",
    "# Split the test data into validation and test sets (50% for validation, 50% for test)\n",
    "df_dev_stormfront, df_test_stormfront = train_test_split(df_test_stormfront, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train_stormfront['data_type']='df_stormfront_train'\n",
    "df_dev_stormfront['data_type']='df_stormfront_dev'\n",
    "df_test_stormfront['data_type']='df_stormfront_test'\n",
    "print('Stormfront dataset dev+test split size:',df_stormfront_devtest_size)\n",
    "print('Stormfront dataset train set size:', len(df_train_stormfront))\n",
    "print('Stormfront dataset dev set size:', len(df_dev_stormfront))\n",
    "print('Stormfront dataset test set size:', len(df_test_stormfront))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalita18twitter full train set size: 3000\n"
     ]
    }
   ],
   "source": [
    "# load the evalita18twitter set\n",
    "file_path_csv_evalita18twitter_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/TW-folder-20230313T173228Z-001/TW-folder/TW-train/haspeede_TW-train.tsv'\n",
    "\n",
    "df_train_evalita18twitter = pd.read_csv(file_path_csv_evalita18twitter_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18twitter.columns=['id','text','hs']\n",
    "# display(df_train_evalita18twitter)\n",
    "df_train_evalita18twitter['data_type'] = 'train_evalita18twitter'\n",
    "print('evalita18twitter full train set size:', len(df_train_evalita18twitter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalita18facebook full train set size: 3000\n"
     ]
    }
   ],
   "source": [
    "# load the evalita18facebook set\n",
    "file_path_csv_evalita18facebook_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2018/FB-folder-20230313T173818Z-001/FB-folder/FB-train/haspeede_FB-train.tsv'\n",
    "\n",
    "df_train_evalita18facebook = pd.read_csv(file_path_csv_evalita18facebook_train, sep='\\t', names=['id','text','hs'])\n",
    "df_train_evalita18facebook['data_type'] = 'train_evalita18facebook'\n",
    "# display(df_train_evalita18facebook)\n",
    "print('evalita18facebook full train set size:', len(df_train_evalita18facebook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalita20 full train set size: 6837\n"
     ]
    }
   ],
   "source": [
    "# load the evalita20 set\n",
    "file_path_csv_evalita20_train = '/home/pgajo/working/data/datasets/Italian/haspeede_evalita/2020/haspeede2_dev/haspeede2_dev_taskAB.tsv'\n",
    "\n",
    "df_train_evalita20 = pd.read_csv(file_path_csv_evalita20_train, sep='\\t', index_col = None)\n",
    "# display(df_train_evalita20)\n",
    "\n",
    "print('evalita20 full train set size:', len(df_train_evalita20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the offenseval_2020 dataset\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# configs = ['ar', 'da', 'en', 'gr', 'tr']\n",
    "# datasets = {}\n",
    "\n",
    "# for config in configs:\n",
    "#     datasets[config] = load_dataset(\"strombergnlp/offenseval_2020\", config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment setup\n",
    "\n",
    "# Set the index of the CUDA device you want to use\n",
    "device_index = -1  # set to -1 for multigpu\n",
    "\n",
    "# set problem type\n",
    "prob_type = 'binary'\n",
    "\n",
    "# set task name\n",
    "task_name = 'incelsis'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_id = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 0\n",
      "Train sets:\n",
      "train_incelsis    3642\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 3642 \n",
      "\n",
      "Dev sets:\n",
      "dev_incelsis    780\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 780 \n",
      "\n",
      "Test sets:\n",
      "test_incelsis    781\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 780 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define dataset combinations\n",
    "metrics_list_names = [\n",
    "    # monolingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_incelsis_5203'],  # 0\n",
    "    ['train_incelsis_5203+train_davidson_sample',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 1\n",
    "    ['train_incelsis_5203+train_hateval_2019_english_sample',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 2\n",
    "    ['train_incelsis_5203+train_davidson_sample+train_hateval_2019_english_sample',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 3\n",
    "    ['train_hateval_2019_english', 'dev_incelsis_5203', 'test_incelsis_5203'],  # 4\n",
    "    ['train_hateval_2019_english+train_davidson',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 5\n",
    "    ['train_incelsis_5203', 'dev_hateval_2019_english',\n",
    "        'test_hateval_2019_english'],  # 6\n",
    "    ['train_davidson', 'dev_incelsis_5203', 'test_incelsis_5203'],  # 7\n",
    "    ['train_incelsis_5203', 'dev_davidson', 'test_davidson'],  # 8\n",
    "    ['train_incelsis_5203+train_davidson+train_hateval_2019_english',\n",
    "        'dev_davidson', 'test_davidson'],  # 9\n",
    "    ['train_incelsis_5203+train_hateval_2019_english',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 10\n",
    "    ['train_hatexplain_binary', 'hatexplain_binary_dev',\n",
    "        'hatexplain_binary_test'],  # 11\n",
    "    ['train_hatexplain_binary', 'dev_incelsis_5203', 'test_incelsis_5203'],  # 12\n",
    "    ['train_incelsis_5203+train_hatexplain_binary',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 13\n",
    "    ['train_incelsis_5203+train_hatexplain_binary+train_hateval_2019_english',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 14\n",
    "    ['train_incelsis_5203+train_stormfront',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 15\n",
    "    ['train_incelsis_5203+train_stormfront+train_hateval_2019_english',\n",
    "        'dev_incelsis_5203', 'test_incelsis_5203'],  # 16\n",
    "\n",
    "    # multilingual\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'],  # 17\n",
    "    ['train_incelsis_5203+train_hateval_2019_english',\n",
    "        'dev_incelsis_5203', 'test_fdb_250'],  # 18\n",
    "    ['train_incelsis_5203+train_hateval_2019_spanish',\n",
    "        'dev_incelsis_5203', 'test_fdb_250'],  # 19\n",
    "    ['train_incelsis_5203+train_hateval_2019_english+train_hateval_2019_spanish',\n",
    "        'dev_incelsis_5203', 'test_fdb_250'],  # 20\n",
    "    ['train_incelsis_5203+train_evalita18facebook',\n",
    "        'dev_incelsis_5203', 'test_fdb_250'],  # 21\n",
    "    ['train_incelsis_5203+train_evalita18twitter',\n",
    "        'dev_incelsis_5203', 'test_fdb_250'],  # 22\n",
    "    ['train_incelsis_5203+train_evalita18facebook+train_evalita18twitter',\n",
    "        'dev_incelsis_5203', 'test_fdb_250'],  # 23\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'],  # 24\n",
    "    ['train_incelsis_5203', 'dev_incelsis_5203', 'test_fdb_250'],  # 25\n",
    "\n",
    "]\n",
    "\n",
    "# set train datasets\n",
    "df_train = pd.DataFrame()\n",
    "\n",
    "if 'incelsis' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train, df_train_incelsis_5203])\n",
    "\n",
    "if 'davidson' in metrics_list_names[metrics_id][0]:\n",
    "    if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "        df_train = pd.concat([df_train, df_train_davidson_sample])\n",
    "    else:\n",
    "        df_train = pd.concat([df_train, df_train_davidson])\n",
    "\n",
    "if 'hateval' in metrics_list_names[metrics_id][0]:\n",
    "    if 'english' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat(\n",
    "                [df_train, df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train, df_train_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][0]:\n",
    "        if 'incelsis' in metrics_list_names[metrics_id][0] and 'sample' in metrics_list_names[metrics_id][0]:\n",
    "            df_train = pd.concat(\n",
    "                [df_train, df_train_hateval_2019_english_sample])\n",
    "        else:\n",
    "            df_train = pd.concat([df_train, df_train_hateval_2019_spanish])\n",
    "\n",
    "if 'train_hatexplain_binary' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train, df_train_hatexplain_binary])\n",
    "\n",
    "if 'train_stormfront' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train, df_train_stormfront])\n",
    "\n",
    "if 'train_evalita18facebook' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train, df_train_evalita18facebook])\n",
    "\n",
    "if 'train_evalita18twitter' in metrics_list_names[metrics_id][0]:\n",
    "    df_train = pd.concat([df_train, df_train_evalita18twitter])\n",
    "\n",
    "df_dev = pd.DataFrame()\n",
    "# set dev datasets\n",
    "if 'dev_incelsis_5203' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev, df_dev_incelsis_5203])\n",
    "\n",
    "if 'dev_davidson' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev, df_dev_davidson])\n",
    "\n",
    "if 'dev_hateval_2019' in metrics_list_names[metrics_id][1]:\n",
    "    if 'english' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev, df_dev_hateval_2019_english])\n",
    "    if 'spanish' in metrics_list_names[metrics_id][1]:\n",
    "        df_dev = pd.concat([df_dev, df_dev_hateval_2019_spanish])\n",
    "\n",
    "if 'dev_hatexplain_binary' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev, df_dev_hatexplain_binary])\n",
    "\n",
    "if 'dev_stormfront' in metrics_list_names[metrics_id][1]:\n",
    "    df_dev = pd.concat([df_dev, df_dev_stormfront])\n",
    "\n",
    "# set test datasets\n",
    "if 'test_incelsis_5203' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_incelsis_5203\n",
    "\n",
    "if 'test_davidson' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_davidson\n",
    "\n",
    "if 'test_hateval_2019' in metrics_list_names[metrics_id][2]:\n",
    "    if 'english' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_english\n",
    "    if 'spanish' in metrics_list_names[metrics_id][2]:\n",
    "        df_test = df_test_hateval_2019_spanish\n",
    "\n",
    "if 'test_hatexplain_binary' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_hatexplain_binary\n",
    "\n",
    "if 'test_stormfront' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_test_stormfront\n",
    "\n",
    "if 'test_fdb_250' in metrics_list_names[metrics_id][2]:\n",
    "    df_test = df_fdb_250\n",
    "\n",
    "df_train = df_train.sample(frac=1)\n",
    "df_dev = df_dev.sample(frac=1)\n",
    "\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize=False))\n",
    "print('Train set length:', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize=False))\n",
    "print('Train set length:', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize=False))\n",
    "print('Train set length:', len(df_dev), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_simple = 'CNN'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained word2vec model\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "w2v_model = Word2Vec.load(\n",
    "    '/home/pgajo/working/VSM_incels.is/2023-02-08_W2V_IFC-22-en.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminary network settings for vectorization\n",
    "maxlen = 100\n",
    "embedding_dims = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding/Truncating: 100%|██████████| 3642/3642 [00:00<00:00, 126811.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding/Truncating: 100%|██████████| 780/780 [00:00<00:00, 84220.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Padding/Truncating: 100%|██████████| 781/781 [00:00<00:00, 93350.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# vectorize train/dev/test sets\n",
    "print('train')\n",
    "x_train = tokenize_and_vectorize_1dlist(df_train['text'], w2v_model.wv)\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(df_train['hs'])\n",
    "print('dev')\n",
    "x_dev = tokenize_and_vectorize_1dlist(df_dev['text'], w2v_model.wv)\n",
    "x_dev = pad_trunc(x_dev, maxlen)\n",
    "x_dev = np.reshape(x_dev, (len(x_dev), maxlen, embedding_dims))\n",
    "y_dev = np.array(df_dev['hs'])\n",
    "print('test')\n",
    "x_test = tokenize_and_vectorize_1dlist(df_test['text'], w2v_model.wv)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(df_test['hs'])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pgajo/working/data/metrics/metrics_monolingual/CNN/0_CNN_2023-03-28_02-15-23_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# filename bits\n",
    "multilingual = 0\n",
    "if multilingual:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_multilingual'\n",
    "else:\n",
    "    metrics_save_path = '/home/pgajo/working/data/metrics/metrics_monolingual'\n",
    "\n",
    "metrics_save_path_model = os.path.join(metrics_save_path, model_name_simple)\n",
    "\n",
    "if not os.path.exists(metrics_save_path_model):\n",
    "    os.mkdir(metrics_save_path_model)\n",
    "\n",
    "metrics_filename = str(metrics_id)+'_'+model_name_simple + \\\n",
    "    '_'+time_str+'_metrics.csv'\n",
    "metrics_csv_filepath = os.path.join(metrics_save_path_model, metrics_filename)\n",
    "print(metrics_csv_filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = 16\n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "threshold = 0.5\n",
    "target_names = ['class 0', 'class 1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from keras.optimizers import Adam\n",
    "from keras_tuner.tuners import Hyperband\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Defining the hyperparameters to explore, based on the CNN from before\n",
    "\n",
    "\n",
    "# # Load the hyperparameters from file\n",
    "# with open(r'C:\\Users\\Paolo\\My Drive\\UNI_Google_Drive\\NLP_Google_Drive\\incels_2022-2023\\best_models\\CNN\\hate_speech\\2023-02-16_best_hyperparameters_CNN_incelsis.json', 'r') as f:\n",
    "#     best_hps = json.load(f)\n",
    "#     # best_hps=best_hps['values']\n",
    "# print(*[\"{}: {}\".format(k, v) for k, v in best_hps.items()], sep=\"\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search HP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential  # Base Keras NN model\n",
    "# Convolution layer and pooling\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Dense, Dropout, Activation, MaxPooling1D, Flatten\n",
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# functions used for evaluation metrics\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 30 Complete [00h 00m 50s]\n",
      "val_acc: 0.808717954158783\n",
      "\n",
      "Best val_acc So Far: 0.872820520401001\n",
      "Total elapsed time: 00h 08m 59s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "def build_loaded_hypermodel(hp):  # to use with json config\n",
    "    # Loss functions: binary_crossentropy\n",
    "    hp_loss = best_hps['values']['loss_function']\n",
    "    # Kernel sizes: from 1 to 5\n",
    "    hp_kernel = best_hps['values']['kernel_size']\n",
    "    # Number of filters: from 50 to 250, with a step of 25 (e.g. it can be 75, 100 etc.)\n",
    "    hp_filters = best_hps['values']['conv_filters']\n",
    "    # Learning rates for the optimizer: 0.002, 0.001, 0.0001\n",
    "    hp_learning_rate = best_hps['values']['adam_learning_rate']\n",
    "    # Number of units in the Dense layer: from 32 to 512, with a step of 32\n",
    "    hp_dense_units = best_hps['values']['dense_units']\n",
    "    # Dropout value: 0.05, 0.1, 0.2, 0.3\n",
    "    hp_dropout = best_hps['values']['dropout_value']\n",
    "    # Intermediate layers: from 1 to 3 sections of Dense layers with Dropout\n",
    "    hp_layers = best_hps['values']['num_intermediate_layers']\n",
    "\n",
    "    model_hp = Sequential()\n",
    "    model_hp.add(Conv1D(filters=hp_filters, kernel_size=hp_kernel, padding='same',\n",
    "                 activation='relu', strides=1, input_shape=(maxlen, embedding_dims)))\n",
    "    model_hp.add(GlobalMaxPooling1D())\n",
    "\n",
    "    for i in range(hp_layers):\n",
    "        model_hp.add(Dense(units=hp_dense_units, activation='relu'))\n",
    "        model_hp.add(Dropout(hp_dropout))\n",
    "\n",
    "    model_hp.add(Activation('relu'))\n",
    "    model_hp.add(Dense(1, activation='sigmoid'))\n",
    "    # model_hp.add(Dense(1,activation=hp_output_activation)) # for regression with raw relu and squeezed into sigmoid\n",
    "\n",
    "    model_hp.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                     loss=hp_loss, metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "    return model_hp\n",
    "\n",
    "# Defining the hyperparameters to explore, based on the CNN from before\n",
    "\n",
    "\n",
    "def build_hypermodel(hp):\n",
    "\n",
    "    # Loss functions: binary_crossentropy\n",
    "    hp_loss = hp.Choice('loss_function', values=['binary_crossentropy'])\n",
    "    # Kernel sizes: from 1 to 3\n",
    "    hp_kernel = hp.Choice('kernel_size', values=[1, 2, 3, 4, 5])\n",
    "    # Number of filters: from 50 to 250, with a step of 25 (e.g. it can be 75, 100 etc.)\n",
    "    hp_filters = hp.Int('conv_filters', min_value=50, max_value=250, step=25)\n",
    "    # Learning rates for the optimizer: 0.002, 0.001, 0.0001\n",
    "    hp_learning_rate = hp.Choice(\n",
    "        'adam_learning_rate', values=[0.002, 0.001, 0.0001])\n",
    "    # Number of units in the Dense layer: from 32 to 512, with a step of 32\n",
    "    hp_dense_units = hp.Int('dense_units', min_value=32,\n",
    "                            max_value=512, step=32)\n",
    "    # Dropout value: 0.05, 0.1, 0.2, 0.3\n",
    "    hp_dropout = hp.Choice('dropout_value', values=[0.05, 0.1, 0.2, 0.3])\n",
    "    # Intermediate layers: from 1 to 3 sections of Dense layers with Dropout\n",
    "    hp_layers = hp.Int('num_intermediate_layers', 1, 3)\n",
    "\n",
    "    model_hp = Sequential()\n",
    "    model_hp.add(Conv1D(filters=hp_filters, kernel_size=hp_kernel, padding='same',\n",
    "                 activation='relu', strides=1, input_shape=(maxlen, embedding_dims)))\n",
    "    model_hp.add(GlobalMaxPooling1D())\n",
    "\n",
    "    for i in range(hp_layers):\n",
    "        model_hp.add(Dense(units=hp_dense_units, activation='relu'))\n",
    "        model_hp.add(Dropout(hp_dropout))\n",
    "\n",
    "    model_hp.add(Activation('relu'))\n",
    "    model_hp.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_hp.compile(optimizer=Adam(learning_rate=hp_learning_rate),\n",
    "                     loss=hp_loss, metrics=['acc', f1_m, precision_m, recall_m])\n",
    "\n",
    "    return model_hp\n",
    "\n",
    "\n",
    "hp = kt.HyperParameters()\n",
    "\n",
    "# Hyperband is one of the optimization algorithms provided by Keras Tuner\n",
    "\n",
    "tuner = Hyperband(build_hypermodel,  # change this if you are using new tuned hyperparameters or if you're loading from json\n",
    "                  # Objective to maximize\n",
    "                  objective=kt.Objective('val_acc', direction='max'),\n",
    "                  executions_per_trial=5,  # Number of models that should be built and fit for each trial\n",
    "                  hyperband_iterations=1,  # The number of times the Hyperband algorithm is iterated over\n",
    "                  max_epochs=10,\n",
    "                  directory=os.path.normpath(date_str+'kt'),\n",
    "                  project_name=\"keras_tuner_project\",\n",
    "                  overwrite=True)\n",
    "\n",
    "tuner.search(x_train, y_train, batch_size=128,  # Batch size -> another parameter that can be explored\n",
    "             epochs=10,\n",
    "             validation_data=(x_dev, y_dev),\n",
    "             # Patience -> number of epochs with no improvement after which training will be stopped; I set it a bit higher to explore more configurations\n",
    "             callbacks=[EarlyStopping('val_loss', patience=3)],\n",
    "             verbose=2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train CNN model with optimal HPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 100, 250)          75250     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 250)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               128512    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               262656    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 466,931\n",
      "Trainable params: 466,931\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "\n",
    "# Get the best model\n",
    "best_model = tuner.get_best_models(1)[0]\n",
    "\n",
    "# Show model summary\n",
    "best_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: 0\n",
      "Train sets:\n",
      "train_incelsis    3642\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 3642 \n",
      "\n",
      "Dev sets:\n",
      "dev_incelsis    780\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 780 \n",
      "\n",
      "Test sets:\n",
      "test_incelsis    781\n",
      "Name: data_type, dtype: int64\n",
      "Train set length: 780 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_prec</th>\n",
       "      <th>val_rec</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>test_rec</th>\n",
       "      <th>model</th>\n",
       "      <th>train_len</th>\n",
       "      <th>train_set(s)</th>\n",
       "      <th>dev_set(s)</th>\n",
       "      <th>test_set(s)</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.204180</td>\n",
       "      <td>0.356396</td>\n",
       "      <td>0.822350</td>\n",
       "      <td>0.751309</td>\n",
       "      <td>0.908228</td>\n",
       "      <td>0.389667</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.736264</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.222411</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.312370</td>\n",
       "      <td>0.815466</td>\n",
       "      <td>0.837545</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.204505</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>0.833819</td>\n",
       "      <td>0.772973</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>0.369996</td>\n",
       "      <td>0.817337</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.209506</td>\n",
       "      <td>0.352008</td>\n",
       "      <td>0.798635</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.359040</td>\n",
       "      <td>0.806569</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.233084</td>\n",
       "      <td>0.308189</td>\n",
       "      <td>0.838213</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.333103</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.802548</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.219503</td>\n",
       "      <td>0.312745</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.881481</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.314485</td>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.208963</td>\n",
       "      <td>0.414782</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.723618</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.428821</td>\n",
       "      <td>0.782482</td>\n",
       "      <td>0.681934</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.234740</td>\n",
       "      <td>0.309805</td>\n",
       "      <td>0.819572</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.329512</td>\n",
       "      <td>0.810373</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.213272</td>\n",
       "      <td>0.315772</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.328718</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>0.844765</td>\n",
       "      <td>0.801370</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.230851</td>\n",
       "      <td>0.337997</td>\n",
       "      <td>0.830303</td>\n",
       "      <td>0.796512</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.339092</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  val_loss    val_f1  val_prec  val_rec  \\\n",
       "0    3.0    0.204180  0.356396  0.822350       0.751309    0.908228   \n",
       "1    3.0    0.222411  0.299213  0.835821       0.878049    0.797468   \n",
       "2    3.0    0.204505  0.357832  0.833819       0.772973    0.905063   \n",
       "3    3.0    0.209506  0.352008  0.798635       0.866667    0.740506   \n",
       "4    3.0    0.233084  0.308189  0.838213       0.816817    0.860759   \n",
       "5    3.0    0.219503  0.312745  0.812287       0.881481    0.753165   \n",
       "6    3.0    0.208963  0.414782  0.806723       0.723618    0.911392   \n",
       "7    3.0    0.234740  0.309805  0.819572       0.792899    0.848101   \n",
       "8    3.0    0.213272  0.315772  0.831169       0.853333    0.810127   \n",
       "9    3.0    0.230851  0.337997  0.830303       0.796512    0.867089   \n",
       "\n",
       "   test_loss   test_f1  test_prec  test_rec model train_len  \\\n",
       "0   0.389667  0.817073        0.736264     0.917808   CNN      3642   \n",
       "1   0.312370  0.815466        0.837545     0.794521   CNN      3642   \n",
       "2   0.369996  0.817337        0.745763     0.904110   CNN      3642   \n",
       "3   0.359040  0.806569        0.863281     0.756849   CNN      3642   \n",
       "4   0.333103  0.831683        0.802548     0.863014   CNN      3642   \n",
       "5   0.314485  0.799270        0.855469     0.750000   CNN      3642   \n",
       "6   0.428821  0.782482        0.681934     0.917808   CNN      3642   \n",
       "7   0.329512  0.810373        0.769231     0.856164   CNN      3642   \n",
       "8   0.328718  0.822496        0.844765     0.801370   CNN      3642   \n",
       "9   0.339092  0.819200        0.768769     0.876712   CNN      3642   \n",
       "\n",
       "           train_set(s)         dev_set(s)         test_set(s)  run_id  \n",
       "0  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "1  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "2  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "3  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "4  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "5  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "6  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "7  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "8  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "9  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_prec</th>\n",
       "      <th>val_rec</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>test_rec</th>\n",
       "      <th>model</th>\n",
       "      <th>train_len</th>\n",
       "      <th>train_set(s)</th>\n",
       "      <th>dev_set(s)</th>\n",
       "      <th>test_set(s)</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.204180</td>\n",
       "      <td>0.356396</td>\n",
       "      <td>0.822350</td>\n",
       "      <td>0.751309</td>\n",
       "      <td>0.908228</td>\n",
       "      <td>0.389667</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.736264</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.222411</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.312370</td>\n",
       "      <td>0.815466</td>\n",
       "      <td>0.837545</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.204505</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>0.833819</td>\n",
       "      <td>0.772973</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>0.369996</td>\n",
       "      <td>0.817337</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.209506</td>\n",
       "      <td>0.352008</td>\n",
       "      <td>0.798635</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.359040</td>\n",
       "      <td>0.806569</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.233084</td>\n",
       "      <td>0.308189</td>\n",
       "      <td>0.838213</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.333103</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.802548</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.219503</td>\n",
       "      <td>0.312745</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.881481</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.314485</td>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.208963</td>\n",
       "      <td>0.414782</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.723618</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.428821</td>\n",
       "      <td>0.782482</td>\n",
       "      <td>0.681934</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.234740</td>\n",
       "      <td>0.309805</td>\n",
       "      <td>0.819572</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.329512</td>\n",
       "      <td>0.810373</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.213272</td>\n",
       "      <td>0.315772</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.328718</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>0.844765</td>\n",
       "      <td>0.801370</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.230851</td>\n",
       "      <td>0.337997</td>\n",
       "      <td>0.830303</td>\n",
       "      <td>0.796512</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.339092</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  val_loss    val_f1  val_prec  val_rec  \\\n",
       "0    3.0    0.204180  0.356396  0.822350       0.751309    0.908228   \n",
       "1    3.0    0.222411  0.299213  0.835821       0.878049    0.797468   \n",
       "2    3.0    0.204505  0.357832  0.833819       0.772973    0.905063   \n",
       "3    3.0    0.209506  0.352008  0.798635       0.866667    0.740506   \n",
       "4    3.0    0.233084  0.308189  0.838213       0.816817    0.860759   \n",
       "5    3.0    0.219503  0.312745  0.812287       0.881481    0.753165   \n",
       "6    3.0    0.208963  0.414782  0.806723       0.723618    0.911392   \n",
       "7    3.0    0.234740  0.309805  0.819572       0.792899    0.848101   \n",
       "8    3.0    0.213272  0.315772  0.831169       0.853333    0.810127   \n",
       "9    3.0    0.230851  0.337997  0.830303       0.796512    0.867089   \n",
       "\n",
       "   test_loss   test_f1  test_prec  test_rec model train_len  \\\n",
       "0   0.389667  0.817073        0.736264     0.917808   CNN      3642   \n",
       "1   0.312370  0.815466        0.837545     0.794521   CNN      3642   \n",
       "2   0.369996  0.817337        0.745763     0.904110   CNN      3642   \n",
       "3   0.359040  0.806569        0.863281     0.756849   CNN      3642   \n",
       "4   0.333103  0.831683        0.802548     0.863014   CNN      3642   \n",
       "5   0.314485  0.799270        0.855469     0.750000   CNN      3642   \n",
       "6   0.428821  0.782482        0.681934     0.917808   CNN      3642   \n",
       "7   0.329512  0.810373        0.769231     0.856164   CNN      3642   \n",
       "8   0.328718  0.822496        0.844765     0.801370   CNN      3642   \n",
       "9   0.339092  0.819200        0.768769     0.876712   CNN      3642   \n",
       "\n",
       "           train_set(s)         dev_set(s)         test_set(s)  run_id  \n",
       "0  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "1  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "2  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "3  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "4  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "5  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "6  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "7  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "8  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "9  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "# write set identifiers for the pandas metrics dataframe\n",
    "df_metrics_train_set_string = ''\n",
    "for i, index in enumerate(df_train['data_type'].value_counts(normalize=False).index.to_list()):\n",
    "    set_len = df_train['data_type'].value_counts(normalize=False).values[i]\n",
    "    df_metrics_train_set_string += index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "df_metrics_dev_set_string = ''\n",
    "for i, index in enumerate(df_dev['data_type'].value_counts(normalize=False).index.to_list()):\n",
    "    set_len = df_dev['data_type'].value_counts(normalize=False).values[i]\n",
    "    df_metrics_dev_set_string += index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "df_metrics_test_set_string = ''\n",
    "for i, index in enumerate(df_test['data_type'].value_counts(normalize=False).index.to_list()):\n",
    "    set_len = df_test['data_type'].value_counts(normalize=False).values[i]\n",
    "    df_metrics_test_set_string += index+'('+str(set_len)+')'+'\\n'\n",
    "\n",
    "# train\n",
    "print('Run ID:', metrics_id)\n",
    "print('Train sets:')\n",
    "print(df_train['data_type'].value_counts(normalize=False))\n",
    "print('Train set length', len(df_train), '\\n')\n",
    "print('Dev sets:')\n",
    "print(df_dev['data_type'].value_counts(normalize=False))\n",
    "print('Train set length', len(df_dev), '\\n')\n",
    "print('Test sets:')\n",
    "print(df_test['data_type'].value_counts(normalize=False))\n",
    "print('Train set length', len(df_dev), '\\n')\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=['epoch', 'train_loss', 'val_loss', 'val_f1', 'val_prec',\n",
    "                          'val_rec', 'test_loss', 'test_f1', 'test_prec', 'test_rec'])\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Training loop\n",
    "for i in range(10):  # run the loop n times\n",
    "\n",
    "    model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "    # Train the model and get the history object\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_dev, y_dev),\n",
    "                        epochs=epochs,\n",
    "                        batch_size=128,\n",
    "                        verbose=1)\n",
    "\n",
    "    # Get the validation loss for each epoch\n",
    "    train_losses = history.history['loss']\n",
    "    train_loss = train_losses[-1]\n",
    "    val_losses = history.history['val_loss']\n",
    "    val_loss = val_losses[-1]\n",
    "\n",
    "    # Compute dev metrics\n",
    "    pred_dev = [el[0] for el in model.predict(x_dev)]\n",
    "    pred_dev = [(el > threshold).astype(\"int32\") for el in pred_dev]\n",
    "\n",
    "    # Calculate val_loss and dev_accuracy\n",
    "    dev_metrics = model.evaluate(x_dev, y_dev, verbose=0)\n",
    "    val_loss, dev_accuracy, *_ = dev_metrics\n",
    "\n",
    "    val_f1 = f1_score(y_dev, pred_dev, average='binary', pos_label=1)\n",
    "    val_prec = precision_score(y_dev, pred_dev)\n",
    "    val_rec = recall_score(y_dev, pred_dev)\n",
    "\n",
    "    # Compute test metrics\n",
    "    pred_test = [el[0] for el in model.predict(x_test)]\n",
    "    pred_test = [(el > threshold).astype(\"int32\") for el in pred_test]\n",
    "\n",
    "    # Calculate val_loss and dev_accuracy\n",
    "    test_metrics = model.evaluate(x_test, y_test, verbose=0)\n",
    "    test_loss, test_accuracy, *_ = test_metrics\n",
    "\n",
    "    test_f1 = f1_score(y_test, pred_test, average='binary', pos_label=1)\n",
    "    test_prec = precision_score(y_test, pred_test)\n",
    "    test_rec = recall_score(y_test, pred_test)\n",
    "\n",
    "    df_metrics = df_metrics.append({\n",
    "        'epoch': epochs,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss,\n",
    "        'val_f1': val_f1,\n",
    "        'val_prec': val_prec,\n",
    "        'val_rec': val_rec,\n",
    "        'test_loss': test_loss,\n",
    "        'test_f1': test_f1,\n",
    "        'test_prec': test_prec,\n",
    "        'test_rec': test_rec\n",
    "    }, ignore_index=True)\n",
    "\n",
    "    df_metrics['model'] = model_name_simple\n",
    "    df_metrics['train_len'] = str(len(df_train))\n",
    "    df_metrics['train_set(s)'] = df_metrics_train_set_string[:-1]\n",
    "    df_metrics['dev_set(s)'] = df_metrics_dev_set_string[:-1]\n",
    "    df_metrics['test_set(s)'] = df_metrics_test_set_string[:-1]\n",
    "    df_metrics['run_id'] = metrics_id\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    print('Run ID:', metrics_id)\n",
    "    print('Train sets:')\n",
    "    print(df_train['data_type'].value_counts(normalize=False))\n",
    "    print('Train set length:', len(df_train), '\\n')\n",
    "    print('Dev sets:')\n",
    "    print(df_dev['data_type'].value_counts(normalize=False))\n",
    "    print('Train set length:', len(df_dev), '\\n')\n",
    "    print('Test sets:')\n",
    "    print(df_test['data_type'].value_counts(normalize=False))\n",
    "    print('Train set length:', len(df_dev), '\\n')\n",
    "\n",
    "    display(df_metrics)\n",
    "\n",
    "# Calculate the average of the statistics over the 5 training iterations\n",
    "average_metrics = df_metrics.loc[:, 'train_loss':'test_rec'].mean(axis=0)\n",
    "\n",
    "# Create a new row with the average statistics\n",
    "average_row = pd.DataFrame(average_metrics).transpose()\n",
    "\n",
    "# Set the 'epoch' value to 'average' for the new row\n",
    "average_row['epoch'] = 'average'\n",
    "\n",
    "# Copy non-numeric columns from the last row of df_metrics to the average_row\n",
    "non_numeric_columns = ['model', 'train_len',\n",
    "                       'train_set(s)', 'dev_set(s)', 'test_set(s)', 'run_id']\n",
    "average_row[non_numeric_columns] = df_metrics.loc[df_metrics.index[-1],\n",
    "                                                  non_numeric_columns]\n",
    "\n",
    "# Append the average row to the df_metrics DataFrame\n",
    "# df_metrics = df_metrics.append(average_row, ignore_index=True)\n",
    "\n",
    "# Display the updated df_metrics DataFrame with the average row\n",
    "display(df_metrics)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_prec</th>\n",
       "      <th>val_rec</th>\n",
       "      <th>test_loss</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>test_rec</th>\n",
       "      <th>model</th>\n",
       "      <th>train_len</th>\n",
       "      <th>train_set(s)</th>\n",
       "      <th>dev_set(s)</th>\n",
       "      <th>test_set(s)</th>\n",
       "      <th>run_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.204180</td>\n",
       "      <td>0.356396</td>\n",
       "      <td>0.822350</td>\n",
       "      <td>0.751309</td>\n",
       "      <td>0.908228</td>\n",
       "      <td>0.389667</td>\n",
       "      <td>0.817073</td>\n",
       "      <td>0.736264</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.222411</td>\n",
       "      <td>0.299213</td>\n",
       "      <td>0.835821</td>\n",
       "      <td>0.878049</td>\n",
       "      <td>0.797468</td>\n",
       "      <td>0.312370</td>\n",
       "      <td>0.815466</td>\n",
       "      <td>0.837545</td>\n",
       "      <td>0.794521</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.204505</td>\n",
       "      <td>0.357832</td>\n",
       "      <td>0.833819</td>\n",
       "      <td>0.772973</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>0.369996</td>\n",
       "      <td>0.817337</td>\n",
       "      <td>0.745763</td>\n",
       "      <td>0.904110</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.209506</td>\n",
       "      <td>0.352008</td>\n",
       "      <td>0.798635</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.740506</td>\n",
       "      <td>0.359040</td>\n",
       "      <td>0.806569</td>\n",
       "      <td>0.863281</td>\n",
       "      <td>0.756849</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.233084</td>\n",
       "      <td>0.308189</td>\n",
       "      <td>0.838213</td>\n",
       "      <td>0.816817</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.333103</td>\n",
       "      <td>0.831683</td>\n",
       "      <td>0.802548</td>\n",
       "      <td>0.863014</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.219503</td>\n",
       "      <td>0.312745</td>\n",
       "      <td>0.812287</td>\n",
       "      <td>0.881481</td>\n",
       "      <td>0.753165</td>\n",
       "      <td>0.314485</td>\n",
       "      <td>0.799270</td>\n",
       "      <td>0.855469</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.208963</td>\n",
       "      <td>0.414782</td>\n",
       "      <td>0.806723</td>\n",
       "      <td>0.723618</td>\n",
       "      <td>0.911392</td>\n",
       "      <td>0.428821</td>\n",
       "      <td>0.782482</td>\n",
       "      <td>0.681934</td>\n",
       "      <td>0.917808</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.234740</td>\n",
       "      <td>0.309805</td>\n",
       "      <td>0.819572</td>\n",
       "      <td>0.792899</td>\n",
       "      <td>0.848101</td>\n",
       "      <td>0.329512</td>\n",
       "      <td>0.810373</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.856164</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.213272</td>\n",
       "      <td>0.315772</td>\n",
       "      <td>0.831169</td>\n",
       "      <td>0.853333</td>\n",
       "      <td>0.810127</td>\n",
       "      <td>0.328718</td>\n",
       "      <td>0.822496</td>\n",
       "      <td>0.844765</td>\n",
       "      <td>0.801370</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.230851</td>\n",
       "      <td>0.337997</td>\n",
       "      <td>0.830303</td>\n",
       "      <td>0.796512</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.339092</td>\n",
       "      <td>0.819200</td>\n",
       "      <td>0.768769</td>\n",
       "      <td>0.876712</td>\n",
       "      <td>CNN</td>\n",
       "      <td>3642</td>\n",
       "      <td>train_incelsis(3642)</td>\n",
       "      <td>dev_incelsis(780)</td>\n",
       "      <td>test_incelsis(781)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  val_loss    val_f1  val_prec  val_rec  \\\n",
       "0    3.0    0.204180  0.356396  0.822350       0.751309    0.908228   \n",
       "1    3.0    0.222411  0.299213  0.835821       0.878049    0.797468   \n",
       "2    3.0    0.204505  0.357832  0.833819       0.772973    0.905063   \n",
       "3    3.0    0.209506  0.352008  0.798635       0.866667    0.740506   \n",
       "4    3.0    0.233084  0.308189  0.838213       0.816817    0.860759   \n",
       "5    3.0    0.219503  0.312745  0.812287       0.881481    0.753165   \n",
       "6    3.0    0.208963  0.414782  0.806723       0.723618    0.911392   \n",
       "7    3.0    0.234740  0.309805  0.819572       0.792899    0.848101   \n",
       "8    3.0    0.213272  0.315772  0.831169       0.853333    0.810127   \n",
       "9    3.0    0.230851  0.337997  0.830303       0.796512    0.867089   \n",
       "\n",
       "   test_loss   test_f1  test_prec  test_rec model train_len  \\\n",
       "0   0.389667  0.817073        0.736264     0.917808   CNN      3642   \n",
       "1   0.312370  0.815466        0.837545     0.794521   CNN      3642   \n",
       "2   0.369996  0.817337        0.745763     0.904110   CNN      3642   \n",
       "3   0.359040  0.806569        0.863281     0.756849   CNN      3642   \n",
       "4   0.333103  0.831683        0.802548     0.863014   CNN      3642   \n",
       "5   0.314485  0.799270        0.855469     0.750000   CNN      3642   \n",
       "6   0.428821  0.782482        0.681934     0.917808   CNN      3642   \n",
       "7   0.329512  0.810373        0.769231     0.856164   CNN      3642   \n",
       "8   0.328718  0.822496        0.844765     0.801370   CNN      3642   \n",
       "9   0.339092  0.819200        0.768769     0.876712   CNN      3642   \n",
       "\n",
       "           train_set(s)         dev_set(s)         test_set(s)  run_id  \n",
       "0  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "1  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "2  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "3  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "4  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "5  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "6  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "7  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "8  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  \n",
       "9  train_incelsis(3642)  dev_incelsis(780)  test_incelsis(781)       0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/pgajo/working/data/metrics/metrics_monolingual/CNN/0_CNN_2023-03-28_02-15-23_metrics.csv'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_csv_filepath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_metrics.to_csv(metrics_csv_filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save HPs to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('_'.join(metrics_csv_filepath.split('_')[:-1])+'_hp.json', 'w') as f:\n",
    "    json.dump(tuner.get_best_hyperparameters(1)[0].get_config(), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgajo-Fz_qUQZq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca09a575e78e41b281752c78b59ffb95d2982f5008f73e33df6571d672f258d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
